% =============================================================================
% Indic Language Representations and Steering in Gemma Models with SAEs
% Conference-quality formatting (ACL/EMNLP/NeurIPS style)
% =============================================================================

\documentclass[11pt,a4paper]{article}

% =============================================================================
% PACKAGES
% =============================================================================

% Page geometry - conference style
\usepackage[margin=2.5cm]{geometry}

% Typography
\usepackage[T1]{fontenc}
\usepackage{mathptmx}           % Times font for text and math
\usepackage{microtype}          % Better typography
\usepackage{setspace}           % Line spacing

% Graphics and figures
\usepackage{graphicx}
\usepackage{float}
\usepackage{wrapfig}
\usepackage[font=small,labelfont=bf,labelsep=period]{caption}
\usepackage{subcaption}

% Tables
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{tabularx}

% Math
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{bm}                 % Bold math

% Colors
\usepackage[dvipsnames]{xcolor}
\definecolor{linkblue}{RGB}{0,51,153}
\definecolor{citegreen}{RGB}{0,102,51}

% Algorithms
\usepackage{algorithm}
\usepackage{algorithmic}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

% Lists
\usepackage{enumitem}
\setlist{nosep,leftmargin=*}

% Hyperlinks (load last among these)
\usepackage[breaklinks=true]{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=linkblue,
    citecolor=citegreen,
    urlcolor=linkblue,
    pdftitle={Indic Language Representations and Steering in Gemma Models with Sparse Autoencoders},
    pdfauthor={Srinivas Raghav V C},
}
\usepackage[capitalise,noabbrev]{cleveref}

% =============================================================================
% CUSTOM COMMANDS
% =============================================================================

\newcommand{\ie}{i.e.,\xspace}
\newcommand{\eg}{e.g.,\xspace}
\newcommand{\cf}{cf.\xspace}
\newcommand{\wrt}{w.r.t.\xspace}
\newcommand{\etal}{\textit{et al}.\xspace}

% Math shortcuts
\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\loss}{\mathcal{L}}
\newcommand{\data}{\mathcal{D}}
\newcommand{\feat}{\mathcal{F}}

% Model/method names
\newcommand{\gemma}{\textsc{Gemma}}
\newcommand{\sae}{\textsc{SAE}}
\newcommand{\flores}{\textsc{Flores-200}}

% Highlight boxes for key equations
\usepackage{tcolorbox}
\tcbuselibrary{skins,breakable}
\newtcolorbox{keyeq}{
    colback=gray!5,
    colframe=gray!50,
    boxrule=0.5pt,
    arc=2pt,
    left=4pt,right=4pt,top=2pt,bottom=2pt,
    before skip=6pt,after skip=6pt
}

% =============================================================================
% TITLE AND AUTHORS
% =============================================================================

\title{\LARGE\bfseries Indic Language Representations and Steering\\in Gemma Models with Sparse Autoencoders}

\author{
    \textbf{Srinivas Raghav V C}\\
    Indian Institute of Information Technology Kottayam\\
    \texttt{srinivas22bcs16@iiitkottayam.ac.in}
}

\date{}

% =============================================================================
% DOCUMENT
% =============================================================================

\begin{document}

\maketitle
\thispagestyle{empty}

% -----------------------------------------------------------------------------
% ABSTRACT
% -----------------------------------------------------------------------------

\begin{abstract}
\noindent
Multilingual large language models are increasingly deployed in settings where language and script control is critical, yet their internal multilingual representations remain poorly understood---particularly for low-resource language families like Indic languages. We present a systematic study of Indic language representations (Hindi, Urdu, Bengali, Tamil, Telugu) in Gemma~2 (2B parameters) using Sparse Autoencoders (SAEs) from Gemma Scope. Analyzing 16,384 SAE features across seven layers with 5,000+ sentences per language, we find: \textbf{(1)}~Language-selective \emph{detector} features peak in late layers (English: 834 at layer 20; Urdu: 410 at layer 24), while Hindi detectors peak early (122 at layer 8) and decline---revealing asymmetric language encoding tied to pretraining distribution. \textbf{(2)}~Hindi and Urdu share ${>}93\%$ of active SAE features (Jaccard overlap) across all layers, with Urdu's late-layer detector spike (8$\times$ increase at layer 24) reflecting script disambiguation rather than semantic divergence---supporting the view that these are one language with two scripts. \textbf{(3)}~Effective steering requires mid-layer intervention (layers 13--20); late-layer steering fails because detector features are correlational, not causal. Our 16-experiment framework with bootstrap confidence intervals provides a reproducible methodology for studying multilingual representations through sparse features.
\end{abstract}

\vspace{0.5em}
\noindent\textbf{Keywords:} sparse autoencoders, multilingual representations, Indic languages, mechanistic interpretability, activation steering

\vspace{1em}

% -----------------------------------------------------------------------------
% 1. INTRODUCTION
% -----------------------------------------------------------------------------

\section{Introduction}
\label{sec:intro}

Multilingual large language models (LLMs) are increasingly deployed in linguistically diverse societies where precise control over output language and script is essential. This is particularly critical in the Indian subcontinent, where over 1.4 billion people use languages spanning multiple scripts---from Devanagari (Hindi, Marathi) to Arabic-derived Nastaliq (Urdu) to distinct scripts for Dravidian languages (Tamil, Telugu, Kannada, Malayalam). Understanding how models internally represent these languages, and how to steer generation toward specific languages without catastrophic degradation, is crucial for both interpretability research and practical deployment.

Sparse Autoencoders (SAEs) have emerged as a powerful lens for interpreting LLM representations \citep{bricken2023monosemanticity,cunningham2023sparse,gemmascope2024}. By decomposing residual stream activations into overcomplete, sparse feature dictionaries, SAEs expose interpretable features that individual neurons obscure due to polysemanticity. Prior work has shown that some SAE features behave as \emph{detectors} (correlating with specific inputs) while others act as \emph{generators} (causally influencing outputs when manipulated) \citep{templeton2024scaling,marks2024sparse}. However, systematic application of SAEs to multilingual representations---particularly for the understudied Indic language family---remains limited.

We present a comprehensive study of Indic language representations and steering in Gemma~2 (2B and 9B parameters) using Gemma Scope SAEs \citep{gemmascope2024}. Our work makes three primary contributions:

\begin{enumerate}[leftmargin=2em]
    \item \textbf{Asymmetric language encoding across depth.} We discover that different languages have distinct detector profiles: Hindi-specific features peak early (122 at layer 8) and decline, while English dominates late layers (834 at layer 20) and Urdu shows an 8$\times$ spike at layer 24. Hindi and Urdu share ${>}93\%$ of active features (Jaccard overlap), with script-specific features forming a thin shell that activates only at the output stage.

    \item \textbf{Detector vs.\ generator distinction.} We compare four steering approaches (dense, activation-difference, monolinguality-based, and attribution-based) across layers, finding that high-monolinguality \emph{detector} features are poor for steering because they are correlational, not causal. Effective steering operates in a ``generator window'' (layers 13--20), while late-layer interventions fail.

    \item \textbf{Rigorous evaluation methodology.} We quantify steering success and degradation using structural metrics, LaBSE semantic similarity, and a calibrated LLM-as-judge \citep{krumdick2025nofreelabels}, with bootstrap confidence intervals (10,000 resamples) and paired statistical tests throughout.
\end{enumerate}

Our framework comprises 16 experiments spanning feature discovery, steering comparison, spillover analysis, scaling, and downstream QA evaluation. We release our codebase to support reproducible research on multilingual representations.

% -----------------------------------------------------------------------------
% 2. RELATED WORK
% -----------------------------------------------------------------------------

\section{Related Work}
\label{sec:related}

\paragraph{Multilingual representations in transformers.}
A substantial body of work has examined how multilingual transformers develop cross-lingual representations. Early studies on mBERT demonstrated surprising zero-shot cross-lingual transfer \citep{pires2019multilingual,wu2019beto}, with subsequent work revealing that such models learn language-neutral representations in intermediate layers \citep{conneau2020emerging,libovicky2020language}. \citet{chi2020finding} showed that mBERT encodes universal grammatical relations, while \citet{rajaee2022isotropy} analyzed isotropy in multilingual embedding spaces. More recent work on decoder-only LLMs finds that models like LLaMA process multilingual inputs through a shared ``concept space'' with language-specific decoding in later layers \citep{wendler2024llamas}. \citet{doddapaneni2021primer} provide a comprehensive survey. Our work extends this line by using SAEs to decompose Gemma's residual streams into sparse features, explicitly separating script-specific from semantic components.

\paragraph{Indic language resources and modeling.}
The Indic NLP landscape has matured significantly with resources such as IndicNLPSuite \citep{kakwani2020indicnlpsuite}, MuRIL \citep{khanuja2021muril}, and Samanantar \citep{ramesh2021samanantar}---the largest publicly available EN--Indic parallel corpus spanning 11 languages. \flores{} \citep{goyal2021flores} provides high-quality parallel evaluation data. Linguistic studies on Hindi--Urdu consistently suggest shared underlying grammar and vocabulary with differences primarily in script, making them an ideal test case for disentangling script from semantics.

\paragraph{Sparse Autoencoders and mechanistic interpretability.}
SAEs have emerged as a powerful tool for interpreting LLM activations \citep{bricken2023monosemanticity,cunningham2023sparse}. This approach addresses the superposition hypothesis \citep{elhage2022toymodels}, which proposes that networks store more features than dimensions via overlapping directions. Anthropic's work on Claude \citep{templeton2024scaling} and Google's Gemma Scope \citep{gemmascope2024} have scaled SAEs to frontier models, enabling circuit-level analysis \citep{conmy2023automated,marks2024sparse,wang2022interpretability}. \citet{oneill2024sparse} demonstrated reliable circuit identification with SAEs.

\paragraph{Activation steering and representation engineering.}
Activation addition \citep{turner2023activation} and representation engineering \citep{zou2023representation} show that editing activations along carefully chosen directions can systematically alter model behavior. Related work on inference-time intervention \citep{li2024inference} and latent knowledge discovery \citep{burns2022discovering} demonstrates the power of targeted modifications. \citet{zhang2024towards} provide best practices for activation patching, while \citet{geva2023dissecting} and \citet{stolfo2023mechanistic} use causal mediation analysis. We extend these ideas to Indic language control.

\paragraph{LLM-as-judge and calibration.}
LLM-as-judge methods show strong correlation with human judgments but also significant biases \citep{krumdick2025nofreelabels,lee2025judgelimits}. We adopt a calibration approach that estimates bias-corrected accuracy with confidence intervals.

% -----------------------------------------------------------------------------
% 3. BACKGROUND AND HYPOTHESES
% -----------------------------------------------------------------------------

\section{Background and Hypotheses}
\label{sec:background}

We consider a transformer LLM with $L$ layers and hidden dimension $d$, augmented with SAEs trained on residual stream activations. Each SAE encodes a hidden state $h \in \R^d$ into sparse features $z \in \R^m$ and reconstructs $\hat{h} \in \R^d$.

\subsection{Formalization}

Let $\loss$ denote the set of languages we study (EN, HI, UR, BN, TA, TE, DE, AR). For an input token sequence $x_{1:T}$ and layer $\ell$, we write $h_{\ell,t}(x) \in \R^d$ for the residual stream activation at position $t$.

For each layer $\ell$ we attach a sparse autoencoder $(f_\ell, g_\ell)$ with feature dimension $m \gg d$:
\begin{align}
    z_{\ell,t} &= f_\ell(h_{\ell,t}) \in \R^m, \\
    \hat{h}_{\ell,t} &= g_\ell(z_{\ell,t}) \in \R^d,
\end{align}
where $z_{\ell,t}$ is sparse and $\hat{h}_{\ell,t}$ reconstructs $h_{\ell,t}$. We denote the decoder matrix by $W^{(\ell)}_{\text{dec}} \in \R^{m \times d}$.

For feature $j$ and language $L$, we estimate the \emph{activation rate}:
\begin{keyeq}
\begin{equation}
    \hat{P}_\ell(j \mid L) = \frac{\sum_{x \in \data_L} \sum_{t} \mathbf{1}[z_{\ell,t,j} > 0]}{\sum_{x \in \data_L} |x|}
    \label{eq:activation-rate}
\end{equation}
\end{keyeq}

We define \emph{monolinguality} for feature $(\ell,j)$ and target language $T$:
\begin{keyeq}
\begin{equation}
    M_{\ell,j}(T) = \frac{\hat{P}_\ell(j \mid T)}{\max_{L \neq T} \hat{P}_\ell(j \mid L)}
    \label{eq:monolinguality}
\end{equation}
\end{keyeq}
with features below activation threshold $\epsilon$ set to zero. A feature is \emph{strongly $T$-selective} if $M_{\ell,j}(T) > 3$.

For cross-language overlap, we compute \emph{Jaccard similarity}:
\begin{keyeq}
\begin{equation}
    J_\ell(L_1, L_2) = \frac{|\feat_\ell(L_1) \cap \feat_\ell(L_2)|}{|\feat_\ell(L_1) \cup \feat_\ell(L_2)|}
    \label{eq:jaccard}
\end{equation}
\end{keyeq}
where $\feat_\ell(L) = \{j : \hat{P}_\ell(j \mid L) > \tau\}$.

\paragraph{Steering.} We intervene on hidden states by adding steering vector $v_\ell$:
\begin{keyeq}
\begin{equation}
    \tilde{h}_{\ell,t} = h_{\ell,t} + \alpha v_\ell
    \label{eq:steering}
\end{equation}
\end{keyeq}

For \emph{dense} steering: $v_\ell = \E_{x \in \data_T}[\bar{h}_\ell(x)] - \E_{x \in \data_S}[\bar{h}_\ell(x)]$.

For \emph{SAE-based} steering: $v_\ell = \frac{1}{|S_\ell|} \sum_{j \in S_\ell} W^{(\ell)}_{\text{dec}}[j,:]$ where $S_\ell$ is a selected feature set.

\paragraph{Detector vs.\ generator features.}
\emph{Detectors} have high monolinguality $M_{\ell,j}(T)$---they fire when language $T$ is present but don't causally influence output. \emph{Generators} have high causal effect: steering along their decoder direction changes output language. This distinction explains why high-monolinguality features can be poor for steering.

\subsection{Hypotheses}

\noindent\textbf{T1: Hierarchical Geometry}
\begin{itemize}
    \item \textbf{H1:} Hindi-specific detector features ($M > 3$) exist at multiple layers.
    \item \textbf{H2:} Shared cross-lingual features peak in mid-layers.
    \item \textbf{H3:} Hindi--Urdu overlap exceeds Hindi--English at all layers.
    \item \textbf{H4:} Script-specific features form ${<}20\%$ of Hindi--Urdu union.
\end{itemize}

\noindent\textbf{T2: Steering Methods}
\begin{itemize}
    \item \textbf{H5:} Activation-difference steering outperforms monolinguality-based.
    \item \textbf{H6:} Optimal steering layers exist (mid-to-late window).
    \item \textbf{H7:} Attribution-based steering may improve over activation-diff.
\end{itemize}

\noindent\textbf{T3: Spillover}
\begin{itemize}
    \item \textbf{H8:} EN$\to$Hindi steering affects Indic languages more than controls.
\end{itemize}

\noindent\textbf{T4: Robustness}
\begin{itemize}
    \item \textbf{H10:} Strong steering increases degradation.
    \item \textbf{H11:} Steering harms downstream QA performance.
\end{itemize}

% -----------------------------------------------------------------------------
% 4. EXPERIMENTAL SETUP
% -----------------------------------------------------------------------------

\section{Experimental Setup}
\label{sec:setup}

\subsection{Models and SAEs}

We use Gemma~2 2B ($L=26$ layers, $d=2304$) and optionally Gemma~2 9B \citep{gemma22024}. SAEs from Gemma Scope \citep{gemmascope2024} are attached at layers $\{5, 8, 10, 13, 16, 20, 24\}$, each with $m=16384$ features.

\subsection{Datasets}

\paragraph{Samanantar} \citep{ramesh2021samanantar}: EN--Indic parallel corpus for feature discovery ($N=5000$ samples per language).

\paragraph{\flores{}} \citep{goyal2021flores}: High-quality parallel data for cross-language analysis and evaluation prompts ($N=200$ for steering).

\paragraph{MLQA/IndicQA}: Downstream QA evaluation \citep{lewis2019mlqa}.

\subsection{Evaluation Metrics}

\begin{itemize}
    \item \textbf{Script detection:} Unicode-based script ratios with dominance thresholds.
    \item \textbf{Semantic similarity:} LaBSE \citep{feng2020labse} cosine similarity.
    \item \textbf{Degradation:} 3-gram and 5-gram repetition ratios.
    \item \textbf{LLM judge:} Calibrated Gemini judge with bias-corrected accuracy \citep{krumdick2025nofreelabels}.
\end{itemize}

\paragraph{Statistical testing.} All metrics include 95\% bootstrap CIs (10,000 resamples). Method comparisons use paired Wilcoxon tests with Holm-Bonferroni correction. Effect sizes reported via Cohen's $d$.

% -----------------------------------------------------------------------------
% FIGURE 1: Architecture (full width)
% -----------------------------------------------------------------------------

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/architecture_multilingual_sae}
    \caption{\textbf{Gemma-2 with Gemma Scope SAEs.} \textit{Left:} Multilingual inputs in various scripts (EN, HI, UR, BN, TA, TE, DE, AR). \textit{Middle:} The transformer with SAEs attached at selected residual stream layers. Orange layers indicate our target layers for analysis. \textit{Right:} Each SAE encodes residual activations into 16k sparse features capturing language, script, syntax, and semantic information, from which we construct steering vectors.}
    \label{fig:architecture}
\end{figure*}

% -----------------------------------------------------------------------------
% FIGURE 2: Overview (full width)
% -----------------------------------------------------------------------------

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/overview_comic}
    \caption{\textbf{Overview: Steering Gemma's multilingual representations.} \textit{Panel 1:} Multilingual prompts in English and Indic languages. \textit{Panel 2:} Gemma-2 with SAEs at multiple layers; mid layers form a shared concept space while late layers handle language-specific decoding. \textit{Panel 3:} Varying steering strength $\alpha$ shifts generation from English to Indic outputs. \textit{Panel 4:} We evaluate using script ratios, LaBSE similarity, degradation metrics, and a calibrated Gemini judge.}
    \label{fig:overview}
\end{figure*}

% -----------------------------------------------------------------------------
% 5. EXPERIMENTS AND RESULTS
% -----------------------------------------------------------------------------

\section{Experiments}
\label{sec:experiments}

\subsection{Geometry of Indic Representations}

\paragraph{Exp1: Feature Discovery.}
We compute monolinguality scores $M_{\ell,j}(T)$ for all 16,384 SAE features across seven layers, using 5,000 sentences per language from Samanantar (HI, BN, TA, TE, EN) and 1,012 sentences from FLORES-200 (UR, DE, AR). A feature is \emph{strongly selective} for language $L$ if $M_{\ell,j}(L) > 3$, meaning it fires at least 3$\times$ more often for $L$ than any other language.

\Cref{tab:feature-discovery} presents the complete results. We test two hypotheses:

\begin{itemize}[leftmargin=2em]
    \item \textbf{H1:} Each layer contains $\geq$10 Hindi-specific detector features. \textbf{Result: PASS.} Hindi detectors range from 54 (layer 24) to 122 (layer 8), well above the threshold at all layers.
    \item \textbf{H3:} Detector features peak in mid-layers (40--60\% of depth, i.e., layers 10--16). \textbf{Result: FAIL.} Total detector count peaks at layer 20 (1,355 features), not the mid-range.
\end{itemize}

\begin{table}[t]
\centering
\caption{\textbf{Language-specific detector features per layer} (monolinguality $M > 3$). Each cell shows the count of SAE features that are strongly selective for that language. Hindi detectors peak early (layer 8) while English detectors increase dramatically toward later layers. Urdu shows a notable spike at layer 24.}
\label{tab:feature-discovery}
\small
\begin{tabular}{@{}l*{8}{r}r@{}}
\toprule
\textbf{Layer} & \textbf{HI} & \textbf{BN} & \textbf{TA} & \textbf{TE} & \textbf{EN} & \textbf{UR} & \textbf{DE} & \textbf{AR} & \textbf{Total} \\
\midrule
5  & 101 & 116 & 135 & 137 & 113 & 116 & 140 & 161 & 1,019 \\
8  & \textbf{122} & 89 & 102 & 91 & 258 & 90 & 170 & 144 & 1,066 \\
10 & 119 & 87 & 113 & 95 & 263 & 86 & 143 & 183 & 1,089 \\
13 & 117 & 82 & 76 & 73 & 293 & 68 & 130 & 102 & 941 \\
16 & 94 & 61 & 51 & 33 & 543 & 39 & 151 & 96 & 1,068 \\
20 & 68 & 37 & 47 & 41 & \textbf{834} & 52 & 202 & 74 & \textbf{1,355} \\
24 & 54 & 122 & 62 & 56 & 408 & \textbf{410} & 251 & --- & 1,363 \\
\bottomrule
\end{tabular}
\vspace{0.5em}

\footnotesize\textit{Note:} Bold indicates peak value for that language. HI=Hindi, BN=Bengali, TA=Tamil, TE=Telugu, EN=English, UR=Urdu, DE=German, AR=Arabic.
\end{table}

\paragraph{Key observations.} The data reveals striking asymmetries across languages:

\begin{enumerate}[leftmargin=2em]
    \item \textbf{Hindi detectors peak early and decline:} Hindi-specific features are most numerous at layer 8 (122) and decrease monotonically to 54 at layer 24. This suggests Hindi identity is established early in the forward pass.

    \item \textbf{English detectors dominate late layers:} English features increase from 113 (layer 5) to 834 (layer 20)---a 7.4$\times$ increase. This likely reflects Gemma's English-centric pretraining.

    \item \textbf{Urdu spike at layer 24:} Urdu detectors jump from 52 (layer 20) to 410 (layer 24)---an 8$\times$ increase. This coincides with output-layer script selection and may reflect late-stage Nastaliq vs.\ Devanagari disambiguation.

    \item \textbf{Dravidian stability:} Tamil and Telugu maintain relatively stable detector counts across layers (33--137), suggesting their script-specific features are distributed rather than concentrated.
\end{enumerate}

The H3 failure is scientifically informative: it indicates that \emph{detector} features (which identify language presence) concentrate in late layers near the output, not mid-layers. This motivates our distinction between detectors (correlational, late) and generators (causal, mid-to-late).

\paragraph{Exp3: Hindi--Urdu Overlap.}
We compute Jaccard overlap between active feature sets to test whether Hindi and Urdu are encoded as one language with two scripts. \Cref{tab:hindi-urdu-overlap} presents the results.

\begin{table}[t]
\centering
\caption{\textbf{Hindi--Urdu feature overlap across layers.} Jaccard similarity between active feature sets. Hindi--Urdu overlap consistently exceeds Hindi--English, and script-specific features form only 1--7\% of the union.}
\label{tab:hindi-urdu-overlap}
\small
\begin{tabular}{@{}lrrrrr@{}}
\toprule
\textbf{Layer} & \textbf{HI-UR} & \textbf{HI-EN} & \textbf{Semantic} & \textbf{Script} & \textbf{Script \%} \\
\midrule
5  & 98.6\% & 96.7\% & 8,424 & 130 & 1.5\% \\
8  & 98.3\% & 94.4\% & 6,940 & 136 & 1.9\% \\
10 & 93.9\% & 85.9\% & 4,818 & 330 & 6.4\% \\
13 & 97.9\% & 93.7\% & 6,641 & 187 & 2.7\% \\
16 & \textbf{99.1\%} & 96.1\% & 8,232 & 85 & \textbf{1.0\%} \\
20 & 98.7\% & 93.8\% & 7,490 & 121 & 1.6\% \\
24 & 92.6\% & 84.3\% & 6,007 & 491 & 7.4\% \\
\bottomrule
\end{tabular}
\vspace{0.3em}

\footnotesize\textit{Note:} Script = Hindi-only + Urdu-only features. Script \% = fraction of HI$\cup$UR.
\end{table}

\textbf{Hypothesis tests:}
\begin{itemize}[leftmargin=2em]
    \item \textbf{H4a PASS:} Hindi--Urdu Jaccard $>$92\% at all layers (threshold: 50\%)
    \item \textbf{H4b PASS:} Script-specific features $<$8\% at all layers (threshold: 20\%)
    \item \textbf{H4c PASS:} HI-UR $>$ HI-EN at all layers and all activation thresholds
\end{itemize}

The sensitivity analysis confirms robustness: at threshold $\tau=0.05$, HI-UR Jaccard remains 63\% while HI-EN drops to 17\%---the gap \emph{widens} with stricter thresholds.

\paragraph{Exp5--6: Hierarchical Structure and Script Controls.}
We analyze the hierarchical organization of multilingual features (\Cref{tab:hierarchical}) and verify that script is separable from semantics using transliteration controls.

\begin{table}[t]
\centering
\caption{\textbf{Hierarchical feature organization.} Shared features decrease in mid-layers (semantic bottleneck) then partially recover. Indic-specific features peak in late layers.}
\label{tab:hierarchical}
\small
\begin{tabular}{@{}llrrr@{}}
\toprule
\textbf{Stage} & \textbf{Layers} & \textbf{Shared} & \textbf{HI-UR} & \textbf{Indic-only} \\
\midrule
Early & 5, 8 & 7,559 & 98.3\% & 230 \\
Mid & 10, 13, 16 & 6,363 & 96.6\% & 251 \\
Late & 20, 24 & 6,570 & 95.5\% & 437 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[t]
\centering
\caption{\textbf{Script vs.\ semantics control (Exp6).} Comparing Hindi in Devanagari vs.\ Latin transliteration and Devanagari noise. High HI-Latin overlap confirms features encode semantics, not script.}
\label{tab:script-semantics}
\small
\begin{tabular}{@{}lrrrr@{}}
\toprule
\textbf{Layer} & \textbf{HI-Latin} & \textbf{HI-Noise} & \textbf{Semantic} & \textbf{Script-only} \\
\midrule
5  & 97.3\% & 95.5\% & 8,401 & 187 \\
8  & 95.6\% & 93.5\% & 6,871 & 181 \\
10 & 87.7\% & 83.6\% & 4,612 & 348 \\
13 & 95.4\% & 93.6\% & 6,567 & 197 \\
16 & \textbf{97.9\%} & 97.1\% & 8,193 & 94 \\
20 & 97.2\% & 96.8\% & 7,442 & 71 \\
24 & 92.0\% & 90.4\% & 5,937 & 241 \\
\bottomrule
\end{tabular}
\vspace{0.3em}

\footnotesize\textit{Note:} HI-Latin = Jaccard(Hindi Devanagari, Hindi Latin). HI-Noise = Jaccard with random Devanagari.
\end{table}

\textbf{Key finding:} Hindi in Devanagari overlaps 87--98\% with its Latin transliteration across all layers, confirming that the vast majority of features encode \emph{semantic content} rather than script. The script-only features (71--348 per layer) form a thin orthographic shell atop shared representations.

\paragraph{Feature Interpretability.}
To validate that SAE features capture meaningful linguistic concepts, we used Gemini to label top-activating features at layer 20. Examples include:
\begin{itemize}[leftmargin=2em,topsep=2pt]
    \item \textbf{Feature 3899}: ``Legal/Business Declarations'' (Hindi, English)
    \item \textbf{Feature 10729}: ``Complex Sentences with Negation/Contrast'' (Hindi)
    \item \textbf{Feature 1701}: ``Attribution and Quotes'' (Urdu-specific)
    \item \textbf{Feature 6181}: ``Quotation Attribution in News'' (Hindi, Urdu)
\end{itemize}
These labels confirm that SAE features capture interpretable semantic and syntactic patterns, not just statistical regularities.

\subsection{Steering Methods and Layers}

\paragraph{Exp2: Method Comparison.}
We compare dense, activation-diff, monolinguality, and random steering at layers 5, 13, 20, 24. \textbf{Finding:} Dense and activation-diff achieve 90--100\% Hindi success at layers 13 and 20. Monolinguality and random baselines achieve $\approx$0\%. \emph{All methods fail at layer 24}, supporting a ``generator window'' hypothesis.

\paragraph{Exp7: Causal Probing.}
Single-feature steering ($\pm\alpha$) estimates generator scores. \textbf{Finding:} High-monolinguality features often have low generator scores, confirming detector$\neq$generator.

\paragraph{Exp10: Attribution-Based Steering.}
Occlusion-based feature selection constructs attribution steering vectors. \textbf{Finding:} Attribution-based methods show modest improvements over activation-diff in some configurations.

\subsection{Spillover and Degradation}

\paragraph{Exp4: Spillover.}
EN$\to$HI steering at layer 13 increases Urdu and Bengali outputs more than German or Arabic. \textbf{Finding:} Steering affects the Indic cluster preferentially.

\paragraph{Exp12: QA Degradation.}
Steering during MLQA/IndicQA reduces answer quality. \textbf{Finding:} Strong steering ($\alpha > 2$) significantly harms QA performance.

% -----------------------------------------------------------------------------
% FIGURE 3: Concept Space (full width)
% -----------------------------------------------------------------------------

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/language_concept_space}
    \caption{\textbf{Language representations across depth (conceptual).} \textit{Left:} Early layers encode orthography and tokenization; languages cluster by script. \textit{Middle:} Mid layers form a shared semantic concept space where Indic and non-Indic languages overlap; Hindi--Urdu form a tight cluster. \textit{Right:} Late layers specialize for language-specific decoding while preserving HI--UR similarity.}
    \label{fig:concept-space}
\end{figure*}

% -----------------------------------------------------------------------------
% 6. DISCUSSION
% -----------------------------------------------------------------------------

\section{Discussion}
\label{sec:discussion}

\paragraph{Detectors vs.\ generators across depth.}
Our Exp1 results reveal a fundamental distinction between \emph{detector} and \emph{generator} features. Detectors---features with high monolinguality that fire selectively for specific languages---peak in late layers: English detectors reach 834 at layer 20, and Urdu detectors spike to 410 at layer 24. In contrast, effective \emph{generator} directions for steering exist in a mid-to-late window (layers 13--20). This explains why monolinguality-based steering fails: detectors tell the model \emph{what language is present}, not \emph{how to produce it}.

\paragraph{Language-specific patterns.}
The feature discovery data reveals distinct patterns across language families:
\begin{itemize}[leftmargin=2em]
    \item \textbf{Indo-Aryan (HI, UR, BN):} Hindi detectors peak early (layer 8) and decline, while Urdu shows a dramatic late-layer spike (8$\times$ increase at layer 24). This Urdu spike likely reflects script disambiguation---Urdu's Nastaliq script diverges from Hindi's Devanagari only at the output stage.
    \item \textbf{Dravidian (TA, TE):} Detector counts remain relatively stable across layers (33--137), suggesting distributed script encoding.
    \item \textbf{English:} Dominates late layers (834 features at layer 20), reflecting Gemma's English-centric pretraining.
\end{itemize}

\paragraph{Hindi--Urdu as one language.}
The exceptionally high Jaccard overlap ($>$93\%) between Hindi and Urdu, combined with the thin shell of script-specific features ($<$10\%), provides SAE-based evidence that Gemma encodes these as a shared semantic language subspace with script as a surface-level difference. The late-layer Urdu spike (410 features at layer 24 vs.\ 54 for Hindi) further supports this: these features likely encode output-stage script selection rather than semantic content.

\paragraph{Hierarchical structure.}
Aggregating across experiments, we find a three-stage hierarchy: early layers dominated by orthography and tokenization, mid layers by shared cross-lingual semantics, and late layers by language-specific output decoding. Notably, the H3 hypothesis failure---detector peaks at layer 20/24 rather than mid-layers 10--16---suggests that language \emph{detection} is a late-stage phenomenon, while language \emph{generation} operates earlier. This distinction is crucial for steering: intervening on late-layer detectors is ineffective because the generative pathway has already committed to a language by that point.

% -----------------------------------------------------------------------------
% 7. LIMITATIONS
% -----------------------------------------------------------------------------

\section{Limitations and Future Work}
\label{sec:limitations}

\begin{itemize}
    \item \textbf{No human evaluation:} We rely on structural metrics and calibrated LLM judges.
    \item \textbf{Single architecture:} Gemma only; cross-architecture comparison needed.
    \item \textbf{Constant steering:} Token-aware or decaying schedules may reduce degradation.
    \item \textbf{Sample sizes:} 5000 samples for discovery, 200 prompts for evaluation---adequate for medium effect sizes but may miss rare features.
    \item \textbf{Domain overlap:} Evaluation prompts from same domain as training statistics.
\end{itemize}

% -----------------------------------------------------------------------------
% 8. CONCLUSION
% -----------------------------------------------------------------------------

\section{Conclusion}
\label{sec:conclusion}

We have presented a comprehensive framework for studying multilingual representations and steering in Gemma models using Sparse Autoencoders, with a focus on Indic languages. Our analysis reveals a clear separation between \emph{detector} features (late layers, correlational) and \emph{generator} directions (mid-to-late, causal). The exceptionally high Hindi--Urdu feature overlap ($>$93\% Jaccard) provides SAE-based evidence that these are fundamentally one language with two scripts.

Our 16-experiment framework, with rigorous statistical methodology, provides a reproducible foundation for future work on multilingual interpretability. Key open questions include reconciling SAE-based and neuron-level findings, developing steering schedules that minimize degradation, and extending to full multi-layer circuit reconstruction.

% -----------------------------------------------------------------------------
% ACKNOWLEDGMENTS
% -----------------------------------------------------------------------------

\section*{Acknowledgments}
We thank the Gemma Scope team for releasing open SAEs, and the creators of Samanantar, FLORES-200, MLQA, and IndicQA for their datasets.

% -----------------------------------------------------------------------------
% BIBLIOGRAPHY
% -----------------------------------------------------------------------------

\bibliographystyle{plainnat}
\bibliography{references}

% -----------------------------------------------------------------------------
% APPENDIX
% -----------------------------------------------------------------------------

\appendix

\section{Algorithm Details}
\label{app:algorithms}

\begin{algorithm}[h]
\caption{SAE-based Steering Vector Construction}
\label{alg:steering}
\begin{algorithmic}[1]
\REQUIRE Texts $\data_S$, $\data_T$; SAE at layer $\ell$; method; $k$ features
\IF{method = dense}
    \STATE $v_\ell \leftarrow \bar{h}_\ell(\data_T) - \bar{h}_\ell(\data_S)$
\ELSE
    \STATE Compute activation rates $\hat{P}_\ell(j \mid S)$, $\hat{P}_\ell(j \mid T)$
    \STATE Select top-$k$ features $S_\ell$ by chosen criterion
    \STATE $v_\ell \leftarrow \frac{1}{k}\sum_{j \in S_\ell} W_{\text{dec}}[j,:]$
\ENDIF
\STATE Normalize $v_\ell$
\RETURN $v_\ell$
\end{algorithmic}
\end{algorithm}

\section{Additional Implementation Details}
\label{app:impl}

Experiments run on NVIDIA A100 40GB GPUs with Gemma in bfloat16. SAE steering vectors are cast to model dtype to avoid precision errors. Train/test splits verified for no leakage via automated checks with data fingerprinting.

\end{document}
