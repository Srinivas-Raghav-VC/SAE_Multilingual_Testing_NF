\documentclass[11pt,twocolumn]{article}

% Core packages
\usepackage{geometry}
\geometry{margin=1in}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath,amssymb}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{enumitem}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{microtype}

% Fonts: Times-like with decent math; Crimson optional if available
\usepackage{times}
% Uncomment the following two lines if the 'crimson' package is installed
% \usepackage{crimson}
% \renewcommand{\familydefault}{\sfdefault}

% Hyperref and clever references
\usepackage{hyperref}
\usepackage{cleveref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
    citecolor=blue,
    pdftitle={Indic Language Representations and Steering in Gemma Models},
    pdfauthor={Srinivas Raghav V C},
}

% Convenience macros
\newcommand{\sae}{\textsc{sae}}
\newcommand{\llm}{\textsc{llm}}
\newcommand{\flores}{\textsc{flores}}
\newcommand{\mlqa}{\textsc{mlqa}}
\newcommand{\indicqa}{\textsc{indicqa}}

\title{\LARGE \textbf{Indic Language Representations and Steering in Gemma Models with Sparse Autoencoders}}

\author{%
  Srinivas Raghav V C \\
  Indian Institute of Information Technology Kottayam \\
  \texttt{srinivas22bcs16@iiitkottayam.ac.in}
}

\date{}

\begin{document}
\maketitle

% NOTE: Abstract will be fully populated after final experiments; current text
% reflects our latest understanding and explicitly scopes our claims.
\begin{abstract}
Multilingual large language models (LLMs) are widely deployed in
settings where language and script control matter, yet their internal
multilingual representations and the effects of steering remain poorly
understood, particularly for Indic languages. We study the Indic cluster
(Hindi, Urdu, Bengali, Tamil, Telugu) in Gemma~2 models (2B and 9B)
using Sparse Autoencoders (SAEs) trained on Gemma residual streams,
following recent work on SAE-based circuit discovery in
LLMs~\cite{oneill2024sparse,gemmascope2024}. In our SAE decomposition
we find that language-selective \emph{detector} features, defined via a
monolinguality score, are most numerous in late layers, while steering
experiments identify a mid-to-late ``generator window'' where
activation-difference and dense steering directions are effective but
final-layer interventions consistently fail. We also observe that Hindi
and Urdu share an exceptionally high fraction of active SAE features
(Jaccard overlap $>$0.93 across probed layers), always exceeding
Hindi--English overlap, with script-specific features forming less than
10\% of the Hindi--Urdu union. Finally, we quantify steering-induced
degradation and downstream QA impact using structural metrics (script
dominance, LaBSE similarity, $n$-gram repetition) alongside a calibrated
Gemini-based LLM-as-judge. We explicitly acknowledge that our
late-layer detector peak may reflect properties of the SAE
factorization and Gemma architecture, and that prior neuron-level work
sometimes finds language selectivity peaking in mid layers; reconciling
these views is left for future work. All experiments are implemented in
an open-source pipeline built around Gemma Scope SAEs and standard
Indic datasets (Samanantar, FLORES-200, MLQA, IndicQA), and this draft
describes the methodology and experimental design in full.
\end{abstract}

\section{Introduction}

Multilingual LLMs are increasingly used in multilingual societies where control over language and script is critical. In the Indic context, models must handle closely related languages (e.g., Hindi and Urdu) that share semantics but differ in script, as well as diverse scripts (Devanagari, Arabic, Bengali, Tamil, Telugu). Understanding how such models internally represent these languages, and how to steer them without catastrophic degradation, is essential both for interpretability and safety.

Sparse Autoencoders (SAEs) have recently emerged as a powerful tool for interpreting LLM activations by decomposing residual stream activations into sparse, interpretable features~\cite{oneill2024sparse}. Prior work has demonstrated that some features behave as ``detectors'' (correlated with certain inputs) while others can act as ``generators'' (causally influencing outputs). However, multilingual and Indic-specific SAE analysis, especially in modern LLMs like Gemma 2, remains under-explored.

We present a comprehensive study of Indic language representations and steering in Gemma 2 (2B and 9B) using SAEs from Gemma Scope~\cite{gemmascope2024}. Our study focuses on four overarching themes:

\begin{itemize}
  \item \textbf{T1: Hierarchical geometry of Indic representations.} How are Indic languages represented across depth? Where do script-specific vs.\ semantic features live? How similar are Hindi--Urdu vs.\ Hindi--English?
  \item \textbf{T2: Steering methods and layers.} Which SAE-based steering constructions (dense, activation-difference, monolinguality, attribution-based) work best, and at what layers, for Indic language control?
  \item \textbf{T3: Spillover and leakage.} How does steering one Indic language affect other Indic languages and unrelated controls?
  \item \textbf{T4: Robustness and task transfer.} How does steering impact degradation and downstream QA performance? Can LLM-as-judge be used reliably when calibrated~\cite{krumdick2025nofreelabels}?
\end{itemize}

We implement a unified experimental framework that operationalizes these questions as a set of experiments (Exp1--Exp16) and generates both plots and a machine-readable summary for analysis. In this draft, all numerical results are left as \textbf{N/A} until the pipeline is fully executed.

\section{Related Work}

\paragraph{Multilingual representations and Indic languages.} 
Prior work has examined how multilingual transformers develop cross-lingual concept spaces, often finding shared mid-layer representations with language-specific decoding in later layers~\cite{wendler2024llamas}. FLORES benchmarks have become standard multilingual testbeds for evaluating translation and representation quality across many languages, including low-resource ones~\cite{goyal2021flores}. Within the Indic space, resources such as IndicNLPSuite~\cite{kakwani2020indicnlpsuite}, MuRIL~\cite{khanuja2021muril}, and Samanantar~\cite{ramesh2021samanantar} have enabled large-scale modelling of Indian languages. Studies on Hindi--Urdu consistently suggest script-specific clustering atop shared semantics. Our work builds on this by using SAEs to decompose Gemma residual streams into sparse features and explicitly separating script vs.\ semantic components.

\paragraph{Sparse Autoencoders, superposition, and steering.}
Recent SAE work has shown that language models' residual streams can be decomposed into overcomplete feature dictionaries that support circuit-level analysis and causal interventions~\cite{oneill2024sparse,gemmascope2024,templeton2024scaling}. This line of work is closely tied to the superposition hypothesis~\cite{elhage2022toymodels}, which proposes that networks represent more features than they have dimensions by storing features in overlapping directions, making individual neurons polysemantic. SAEs help disentangle such superposition and expose more interpretable, sparse feature bases, and have been used to improve representational alignment measurements between models and brains~\cite{longon2025superpositionalign}. Representation engineering and steering methods---including activation addition and related activation-editing techniques~\cite{turner2023activation}---show that editing activations along carefully chosen directions can systematically change model behavior but may introduce degradation (repetition, incoherence). Our study extends these ideas to a structured, multi-language Indic setting in Gemma models, using Gemma Scope SAEs to study both language representations and steering-induced trade-offs.

\paragraph{LLM-as-judge and calibration.}
LLM-as-judge methods use large models to evaluate generated text. Recent papers show strong correlation with human judgements but also highlight significant biases and the need for calibration or human grounding~\cite{krumdick2025nofreelabels,lee2025judgelimits}. We adopt a calibration approach in the spirit of Lee et al., estimating bias-corrected accuracy and confidence intervals for a Gemini-based judge rather than treating raw judge scores as ground truth.

\paragraph{Indic corpora.}
Samanantar~\cite{ramesh2021samanantar} is, to our knowledge, the largest publicly available EN--Indic parallel corpus, spanning 11 Indic languages and widely used for multilingual translation and representation learning~\cite{kakwani2020indicnlpsuite,khanuja2021muril}. FLORES-200~\cite{goyal2021flores} provides high-quality parallel evaluation data for a large number of languages, including many Indic languages. We use Samanantar primarily for feature discovery and steering vector estimation, and FLORES-200 for parallel evaluation and cross-lingual analysis. For English, we reuse the same set of FLORES sentences both to estimate steering directions and as test prompts; although these uses occur in different modalities (hidden-state statistics vs.\ generated outputs), we note this reuse explicitly as a limitation.

\section{Background and Hypotheses}

We consider a transformer LLM with $L$ layers and hidden dimension $d$, augmented with SAEs trained on residual stream activations for each layer. Each SAE encodes a hidden state $h \in \mathbb{R}^d$ into sparse features $z \in \mathbb{R}^m$ and reconstructs back to $\hat{h} \in \mathbb{R}^d$.

\subsection{Formalization}

Let $\mathcal{L}$ denote the set of languages we study (EN, HI, UR, BN, TA, TE, DE, AR). For an input token sequence $x_{1:T}$ and a transformer layer index $\ell \in \{0,\dots,L\}$, we write
\begin{equation}
  h_{\ell,t}(x_{1:T}) \in \mathbb{R}^d
\end{equation}
for the residual stream activation at position $t$ and layer $\ell$, and collect all positions into $H_\ell(x) \in \mathbb{R}^{T \times d}$.

For each layer $\ell$ we attach a sparse autoencoder $(f_\ell, g_\ell)$ with feature dimension $m \gg d$:
\begin{align}
  z_{\ell,t} &= f_\ell\!\bigl(h_{\ell,t}\bigr) \in \mathbb{R}^m, \\
  \hat{h}_{\ell,t} &= g_\ell(z_{\ell,t}) \in \mathbb{R}^d,
\end{align}
where $z_{\ell,t}$ is sparse (most coordinates zero) and $\hat{h}_{\ell,t}$ aims to reconstruct $h_{\ell,t}$. We denote the decoder matrix by $W^{(\ell)}_{\mathrm{dec}} \in \mathbb{R}^{m \times d}$, whose $j$-th row gives a direction in hidden space associated with feature $j$ at layer $\ell$.

For a fixed layer $\ell$ and feature index $j$, we define a binary activation indicator at token position $(x,t)$
\begin{equation}
  a_{\ell,j}(x,t) \;=\; \mathbb{I}\bigl[z_{\ell,t,j} > 0\bigr],
\end{equation}
where $\mathbb{I}[\cdot]$ is the indicator function. For a language $L \in \mathcal{L}$ with a corpus $\mathcal{D}_L$ of sentences, we estimate the \emph{activation rate} of feature $(\ell,j)$ given $L$ as
\begin{equation}
  \hat{P}_\ell(j \mid L) \;=\;
  \frac{\displaystyle \sum_{x \in \mathcal{D}_L} \sum_{t=1}^{|x|} a_{\ell,j}(x,t)}
       {\displaystyle \sum_{x \in \mathcal{D}_L} |x|},
\end{equation}
which is simply the fraction of tokens for which feature $j$ is active on layer $\ell$ for language $L$. In practice, $\mathcal{D}_L$ is drawn from Samanantar (for Indic languages with EN parallels) or FLORES-200 (for languages not covered by Samanantar).

Given these activation rates, we define a \emph{language-selectivity score} (``monolinguality'') for feature $(\ell,j)$ and target language $T \in \mathcal{L}$ as
\begin{equation}
  M_{\ell,j}(T) \;=\;
  \begin{cases}
    0, & \text{if } \hat{P}_\ell(j \mid T) < \epsilon, \\[4pt]
    \dfrac{\hat{P}_\ell(j \mid T)}{\max_{L \in \mathcal{L},\, L \neq T} \hat{P}_\ell(j \mid L)}, & \text{otherwise,}
  \end{cases}
  \label{eq:monolinguality}
\end{equation}
with a small floor $\epsilon$ (e.g., $10^{-4}$) to avoid assigning large scores to features that almost never fire on $T$. We call a feature \emph{strongly $T$-selective} if $M_{\ell,j}(T) > 3$, i.e., it is at least three times more active on $T$ than on any other language in $\mathcal{L}$. Exp1 counts such features per layer and language.

For cross-language feature overlap (Exp3, Exp5, Exp6), we work with sets of active features. For a given layer $\ell$ and language $L$, we define the set
\begin{equation}
  \mathcal{F}_\ell(L) \;=\; \bigl\{ j \in \{1,\dots,m\} : \hat{P}_\ell(j \mid L) > \tau \bigr\},
\end{equation}
where $\tau$ is an activation-rate threshold (e.g., $0.01$). For two languages $L_1, L_2$ we then compute the \emph{Jaccard overlap}
\begin{equation}
  J_\ell(L_1, L_2) \;=\;
  \frac{\bigl|\mathcal{F}_\ell(L_1) \cap \mathcal{F}_\ell(L_2)\bigr|}
       {\bigl|\mathcal{F}_\ell(L_1) \cup \mathcal{F}_\ell(L_2)\bigr|},
  \label{eq:jaccard}
\end{equation}
which is guaranteed to lie in $[0,1]$ by construction and captures how similar the active feature sets are for $L_1$ and $L_2$ at layer $\ell$.

Steering experiments (Exp2, Exp4, Exp7, Exp8, Exp9, Exp10, Exp15, Exp16) intervene on hidden states at layer $\ell$ by adding a \emph{steering vector} $v_\ell \in \mathbb{R}^d$. For a given residual stream state $h_{\ell,t}$ and scalar strength $\alpha \in \mathbb{R}$, the steered hidden state is
\begin{equation}
  \tilde{h}_{\ell,t} \;=\; h_{\ell,t} + \alpha v_\ell.
  \label{eq:steering}
\end{equation}
In the \emph{dense} case, $v_\ell$ is constructed as a difference between mean hidden states for a target language $T$ and a source language $S$:
\begin{equation}
  v_\ell^{\mathrm{dense}}(T \leftarrow S) \;=\;
  \mathbb{E}_{x \in \mathcal{D}_T}\bigl[\bar{h}_\ell(x)\bigr]
  \;-\;
  \mathbb{E}_{x \in \mathcal{D}_S}\bigl[\bar{h}_\ell(x)\bigr],
\end{equation}
where $\bar{h}_\ell(x)$ denotes the mean of $H_\ell(x)$ over tokens. In the SAE-based case, we instead select a set of features $S_\ell \subseteq \{1,\dots,m\}$ (e.g., by activation difference or monolinguality) and form
\begin{equation}
  v_\ell^{\mathrm{SAE}} \;=\;
  \frac{1}{|S_\ell|}
  \sum_{j \in S_\ell} W^{(\ell)}_{\mathrm{dec}}[j,:],
  \label{eq:sae-steering}
\end{equation}
which averages the decoder directions of selected features. Occlusion-based attribution (Exp10, Exp13) instead uses the SAE to \emph{ablate} individual or grouped features by encoding $h_{\ell,t}$, zeroing selected coordinates in $z_{\ell,t}$, and decoding back to a modified $\hat{h}_{\ell,t}$.

We will use the terms \emph{detector} and \emph{generator} feature in a precise, operational sense.

\paragraph{Detector features.}
Intuitively, a feature behaves as a detector for language $T$ if it reliably \emph{fires when text is in $T$ and remains mostly silent otherwise}. Formally, we treat the monolinguality score $M_{\ell,j}(T)$ from \eqref{eq:monolinguality} as a \emph{detection score}: features with high $M_{\ell,j}(T)$ and non-trivial activation rate on $T$ are \emph{language-selective detectors}. Exp1 and Exp3 analyse detector structure by:
\begin{itemize}
  \item counting strongly $T$-selective features ($M_{\ell,j}(T) > 3$) per layer, and
  \item comparing overlaps of active detector sets across languages via $J_\ell(L_1,L_2)$ in \eqref{eq:jaccard}.
\end{itemize}
These quantities are purely \emph{correlational}: they say when a feature is \emph{present}, not what happens if we change it.

\paragraph{Generator features.}
By contrast, a feature behaves as a generator for language $T$ if \emph{increasing its activation causally pushes the model toward generating $T$}. We formalize this via single-feature steering and occlusion. For a feature $(\ell,j)$ with decoder direction $w_{\ell,j}$, consider the intervention
\begin{equation}
  h_{\ell,t}^{(+)} \;=\; h_{\ell,t} + \alpha w_{\ell,j}, \qquad
  h_{\ell,t}^{(-)} \;=\; h_{\ell,t} - \alpha w_{\ell,j},
  \label{eq:single-feature-steer}
\end{equation}
for a small $\alpha > 0$, applied during generation. Let $y^{(+)}$ and $y^{(-)}$ be the corresponding completions for a prompt $x$, and let $S_T(y)$ denote a scalar \emph{language score} for $T$ (e.g., the fraction of alphabetic characters in the target script, or an indicator that the output passes our script-dominance test). We define a \emph{generator score} for $(\ell,j)$ with respect to $T$ as
\begin{equation}
  G_{\ell,j}(T) \;=\;
  \mathbb{E}_{x \sim \mathcal{P}}\bigl[ S_T\bigl(y^{(+)}(x)\bigr) - S_T\bigl(y^{(-)}(x)\bigr) \bigr],
  \label{eq:generator-score}
\end{equation}
where the expectation is over a prompt distribution $\mathcal{P}$ (e.g., FLORES prompts). In words, $G_{\ell,j}(T)$ measures how much nudging feature $j$ up versus down changes the tendency of the model to produce language $T$. In practice (Exp7), we approximate \eqref{eq:generator-score} using a finite set of prompts and script-based success indicators.

Occlusion-based attribution in Exp10 provides a complementary, ablation-style generator score. For feature $(\ell,j)$ we define
\begin{equation}
  A_{\ell,j}(T) \;=\;
  \mathbb{E}_{x \sim \mathcal{P}}
  \bigl[ S_T(y(x)) - S_T(\tilde{y}_{-j}(x)) \bigr],
  \label{eq:attribution-score}
\end{equation}
where $y(x)$ is the baseline completion and $\tilde{y}_{-j}(x)$ is the completion when we encode $h_{\ell,t}$ through the SAE, set $z_{\ell,t,j}\!=\!0$ for all $t$, and decode back before continuing the forward pass. A large positive $A_{\ell,j}(T)$ means that \emph{removing} feature $j$ reduces the model's ability to generate $T$; such features are strong candidates for inclusion in attribution-based steering sets $S_\ell^{\mathrm{attr}}(T,k)$.

In this paper we use:
\begin{itemize}
  \item high-$M_{\ell,j}(T)$ features as \emph{detectors} for language $T$ (Exp1, Exp3, Exp5, Exp6), and
  \item high-$G_{\ell,j}(T)$ or high-$A_{\ell,j}(T)$ features as \emph{generators} for $T$ (Exp2, Exp4, Exp7, Exp8, Exp9, Exp10, Exp15, Exp16),
\end{itemize}
with thresholds chosen empirically. This detector/generator distinction clarifies why monolinguality-based feature sets can be excellent for analysis yet poor for steering: detectors tell us \emph{when} a language is present, whereas generators tell us \emph{how to make it appear}.  

For evaluation, we combine several metrics. Script dominance is computed via Unicode-based script ratios over generated text. Semantic preservation is measured using LaBSE cosine similarity between prompt or gold-answer embeddings and generated outputs. Degradation is quantified via $n$-gram repetition rates (e.g., 3-gram and 5-gram). Finally, a calibrated Gemini-based LLM-as-judge provides language, faithfulness, and coherence scores with bias-corrected accuracy estimates~\cite{krumdick2025nofreelabels}. Subsequent sections instantiate these formal ingredients for each experiment.

We study the following grouped hypotheses:

\subsection{T1: Hierarchical Geometry of Indic Representations}

\begin{itemize}
  \item \textbf{H1: Hindi-specific detector features exist at multiple layers.} There are at least a threshold number of Hindi-specific features (monolinguality $M > 3$) per layer.
  \item \textbf{H2: Mid-layers form a shared cross-lingual space.} Shared features across languages peak in mid-layers, consistent with a ``messy middle'' concept space.
  \item \textbf{H3: Hindi--Urdu share more semantic features than Hindi--English.} Jaccard overlap of active features is higher for Hindi--Urdu than for Hindi--English at most layers.
  \item \textbf{H4: Script-specific features form a minority of Hindi--Urdu union.} Script-only features make up less than a given fraction (e.g., $< 20\%$) of the joint feature set.
\end{itemize}

\subsection{T2: Steering Methods and Layers}

\begin{itemize}
  \item \textbf{H5: Activation-difference SAE steering outperforms monolinguality.} For EN$\to$Indic steering, activation-difference SAE features yield higher target-language success than monolinguality-selected features.
  \item \textbf{H6: There exist optimal steering layers for each Indic language.} A subset of layers (often late) yield significantly higher steering success with lower degradation.
  \item \textbf{H7: Attribution-based steering may provide modest improvements over naive activation-diff.} Feature sets selected via occlusion-based attribution can yield small steering gains over purely activation-difference sets in some configurations; we treat this as an exploratory hypothesis rather than a guaranteed improvement.
\end{itemize}

\subsection{T3: Spillover and Leakage}

\begin{itemize}
  \item \textbf{H8: Steering a target Indic language primarily affects the Indic cluster.} EN$\to$Hindi steering increases Hindi usage and also affects other Indic languages more than non-Indic controls.
  \item \textbf{H9: Steering profile similarity is higher within the Indic cluster.} Layer-wise steering success curves are more correlated among Indic languages than between Indic and non-Indic languages.
\end{itemize}

\subsection{T4: Robustness and Task Transfer}

\begin{itemize}
  \item \textbf{H10: Steering induces measurable degradation.} Strong steering increases repetition and coherence degradation.
  \item \textbf{H11: Steering harms downstream QA performance.} Applying EN$\to$Indic steering during QA reduces QA quality (MLQA, IndicQA) relative to baseline.
  \item \textbf{H12: Calibrated LLM-as-judge yields more reliable estimates than raw scores.} Bias-corrected estimates differ from raw Gemini judge accuracy and provide tighter error bounds~\cite{krumdick2025nofreelabels}.
\end{itemize}

\section{Experimental Setup}

\subsection{Models and SAEs}

We primarily use Gemma 2B and optionally Gemma 2 9B~\cite{gemma22024}:
\begin{itemize}
  \item \textbf{Gemma 2 2B}: $L = 26$ layers, $d = 2304$.
  \item \textbf{Gemma 2 9B}: larger model with corresponding Gemma Scope SAEs.
\end{itemize}

SAEs are loaded from Gemma Scope releases and attached to residual streams at selected layers (e.g., 5, 8, 10, 13, 16, 20, 24). For each layer, the SAE has width $m = 16384$ features (width 16k), similar in spirit to SAE configurations used in prior circuit-identification work~\cite{oneill2024sparse}.

\subsection{Datasets}

\paragraph{Samanantar.}
We use the Samanantar EN--Indic parallel corpus~\cite{ramesh2021samanantar} for feature discovery and steering vector estimation, sampling up to $N_\text{train}$ sentence pairs per language. Indic languages include Hindi, Bengali, Tamil, Telugu; English serves as a pivot.

\paragraph{FLORES-200.}
FLORES-200 is a multilingual evaluation benchmark~\cite{goyal2021flores} that provides high-quality parallel data for 100+ languages. In all experiments we use FLORES-200 for:
\begin{itemize}
  \item Cross-language feature overlap and hierarchy (Exp3, Exp5, Exp6).
  \item Additional training data for languages not present in Samanantar (e.g., Urdu, German, Arabic).
\item English prompts for steering evaluation. We augment our small set of 25 handwritten evaluation prompts with additional English sentences from the FLORES-200 `devtest` split to reach a total of $N=200$ prompts per experiment. This ensures that steering success rates and calibrated judge estimates have sufficient sample size for statistical validity. We note that while these sentences are distinct from the training set (which uses the `train` split or Samanantar), they are drawn from the same underlying domain as the source-language statistics used to build the steering vectors. We treat this as valid data reuse (evaluating generation on held-out prompts from the source domain) rather than leakage.
\end{itemize}

\paragraph{MLQA and IndicQA.}
For downstream QA evaluation:
\begin{itemize}
  \item \textbf{MLQA}: EN, HI, DE, AR.
  \item \textbf{IndicQA}: HI, BN, TA, TE.
\end{itemize}
We use these for baseline vs.\ steered QA comparison (Exp12).

\subsection{Evaluation Metrics}

\paragraph{Script detection and code-mixing.}
We compute per-script ratios over alphabetic characters using Unicode ranges (Latin, Devanagari, Arabic, Bengali, Tamil, Telugu, etc.). A generation is classified as target script if:
\begin{enumerate}
  \item Target script ratio $\geq \tau_\text{script}$, and
  \item Target script ratio exceeds the next-largest script ratio by a dominance margin.
\end{enumerate}
We also detect code-mixing by checking if multiple scripts exceed a small threshold.

\paragraph{Semantic similarity (LaBSE).}
We use the LaBSE model~\cite{feng2020labse} to embed prompts or gold answers and generated outputs, computing cosine similarity. This is used both for steering prompts and QA answers.

\paragraph{Degradation (repetition).}
We compute 3-gram and 5-gram repetition ratios and classify outputs as degraded if either exceeds predefined thresholds.

\paragraph{LLM-as-judge (Gemini).}
We use a Gemini model as a judge with a structured rubric (language, faithfulness, coherence). Raw scores are then calibrated using a bias-correction framework inspired by recent work on limitations of LLM-as-a-judge~\cite{krumdick2025nofreelabels}, estimating sensitivity, specificity, and bias-adjusted accuracy with confidence intervals.

\section{Experiments}

In this draft, all numerical results are placeholders (N/A). The codebase provides a unified runner (\texttt{run.py}) and a summarizer (\texttt{summarize\_results.py}). Executing:
\begin{verbatim}
python run.py --all
python summarize_results.py
\end{verbatim}
will generate JSON outputs, plots, and a text summary to populate the results below.

\begin{figure}[t]
  \centering
  % Save the infographic image as, e.g., figures/indic_sae_infographic.(pdf|png|jpg)
  \includegraphics[width=\textwidth]{figures/indic_sae_infographic}
  \caption{High-level overview of our experimental setup. Left: Gemma 2B/9B transformer with Gemma Scope SAEs attached to residual streams at selected layers, processing inputs in multiple scripts (Latin, Devanagari, Arabic, Bengali, Tamil, Telugu) using Samanantar, FLORES-200, MLQA, and IndicQA. Middle: feature analysis via monolinguality-based detectors (Exp1), Hindi--Urdu overlap and script vs semantic splits (Exp3, Exp6), and cross-layer EN--HI alignment (Exp14). Right: steering and evaluation pipeline, including EN$\to$Indic steering with dense and SAE-based directions (Exp2, Exp7, Exp9, Exp10), spillover analysis (Exp4), scaling to 9B (Exp8), QA degradation (Exp12), and calibrated LLM-as-judge evaluation (Exp11), together with robustness metrics (script dominance, LaBSE semantics, repetition).}
  \label{fig:overview}
\end{figure}

\subsection{Exp1: Feature Discovery (Monolinguality Detectors)}

\textbf{Goal.} Identify language-selective detector features by computing monolinguality scores per SAE feature and language, and test whether such features peak in mid layers or late layers.

\textbf{Method.} For each feature $j$ and language $L$, we estimate an activation rate $P(j \mid L)$ as the fraction of tokens on which $j$ is active in Samanantar/FLORES samples. For a target language $T$, we define a monolinguality score
\begin{equation}
  M_j(T) \;=\; \frac{P(j \mid T)}{\max_{L \neq T} P(j \mid L)},
\end{equation}
zeroing out features that almost never fire on $T$ (very small $P(j \mid T)$). A feature is counted as \emph{strongly $T$-selective} if $M_j(T) > 3$, i.e., it is at least three times more active on $T$ than on any other language in our set (EN, HI, UR, BN, TA, TE, DE, AR). We then count, for each layer, how many features are strongly selective for \emph{any} language under this definition.

Our original hypothesis (H3) was that language-specific detector features would peak in a ``mid'' band (layers 10--16), reflecting a mid-layer ``messy middle'' where language identity is maximally separable before decoding. In contrast, our first full run on Gemma~2B shows that the total number of strongly language-selective features \emph{increases toward the top of the network}, with a clear peak at the final SAE layer (one run yields roughly $8{\times}10^2$ selective features at layer~5, rising monotonically to almost $2{\times}10^3$ at layer~24). Under this detector metric, H3 is \emph{falsified}: language selectivity is strongest in late layers rather than in the 10--16 band. At the same time, for each Indic language individually (e.g., Hindi) we still observe dozens of strongly selective detectors at every probed layer, confirming H1.

\textbf{Interpretation.} This pattern suggests that, in Gemma~2B \emph{under our SAE factorization}, features that behave as \emph{language or script detectors} are concentrated in late residual layers closer to the logits. This is in contrast to some neuron-level studies which report mid-layer peaks in language selectivity~\cite{wendler2024llamas}; reconciling SAE-based and neuron-based views is an interesting open question. Mid layers may still host a more language-agnostic semantic space (as probed in Exp14), but the features that are most sharply tuned to specific languages in our decomposition emerge in the later ``decoding'' region. The fact that monolinguality-based detectors become more numerous toward the top also explains why they are informative for analysis (Exp3/Exp6) yet, as Exp2 shows, ineffective as steering directions: they tell the model \emph{what language is present}, not \emph{how to change language}.

\textbf{Results (qualitative).}
\begin{itemize}
  \item Strongly language-selective features (M $>3$) exist for all languages at all probed layers; their total count per layer increases monotonically from early to late layers, peaking at the last SAE layer.
  \item For Hindi specifically, each layer has on the order of $10$--$50$ strongly Hindi-selective detector features, satisfying H1.
  \item Dead-feature counts (features that never activate) are substantial but do not qualitatively change this trend; they are highest for low-resource languages at early layers.
\end{itemize}

\subsection{Exp3: Hindi--Urdu Overlap (Correct Jaccard)}

\textbf{Goal.} Quantify overlap between Hindi and Urdu features and separate script vs.\ semantic features.

\textbf{Method.} For each layer, we collect active features for HI, UR, and EN on FLORES-200, using a fixed activation threshold to define sets $\mathcal{F}_\ell(\text{HI})$, $\mathcal{F}_\ell(\text{UR})$, $\mathcal{F}_\ell(\text{EN})$. We then compute Jaccard overlaps $J_\ell(\text{HI},\text{UR})$ and $J_\ell(\text{HI},\text{EN})$, and decompose the HI--UR union into semantic vs.\ script-specific components by checking whether features are also active for EN.

\textbf{Results (qualitative).}
\begin{itemize}
  \item Across all probed layers, Hindi and Urdu share a very large fraction of active SAE features: $J_\ell(\text{HI},\text{UR})$ is consistently high (in our first run it stays in the $\approx 0.93$--$0.99$ range) and \emph{always} exceeds $J_\ell(\text{HI},\text{EN})$ at the same layer.
  \item The HI--EN Jaccard remains substantial but lower than HI--UR (typically by several percentage points), reflecting that English shares many semantic features with Hindi but is less tightly coupled than Urdu.
  \item When we classify features in the HI--UR union as ``semantic'' (also active for EN) vs.\ ``script-specific'' (active only for HI or UR), we find that semantic features dominate: at all layers the semantic subset accounts for $>90\%$ of the union, while script-specific features form a thin shell (roughly $1$--$7\%$ depending on layer).
  \item Script-specific fractions are smallest in early and mid layers and become slightly larger in the very top SAE layer, consistent with the idea that later layers specialize for script and surface-form decoding.
\end{itemize}

\subsection{Exp5 \& Exp6: Hierarchical and Script vs Semantic Structure}

\textbf{Goal.} Analyze shared vs Indic-only vs script/semantic features across layers, and cluster Indic vs non-Indic languages.

\textbf{Method.} We compute per-language active feature sets for EN, HI, UR, BN, TA, TE, DE, AR and measure:
\begin{itemize}
  \item shared features across all languages,
  \item Indic-cluster features that are not active for non-Indic controls (DE, AR, EN),
  \item and, in Exp6, script-sensitive vs script-robust features by comparing Hindi in Devanagari vs transliterated Latin vs Devanagari ``noise'' strings.
\end{itemize}
We summarize statistics over early (layers 5, 8), mid (10, 13, 16), and late (20, 24) bands.

\textbf{Results (qualitative).}
\begin{itemize}
  \item \emph{Shared structure across depth.} At all layers there is a large pool of features active across all languages, with only modest variation in size between early, mid, and late bands. This suggests a persistent language-agnostic backbone that supports cross-lingual semantics.
  \item \emph{Indic cluster cohesion.} Within this backbone, Indic languages (HI, UR, BN, TA, TE) share additional features that are rarely active for DE or AR. Hindi--Urdu, in particular, maintain very high overlap even when we look at Indic-only feature sets, reinforcing the Exp3 finding that they are encoded as a single language with distinct scripts.
  \item \emph{Script vs semantic structure.} In Exp6, Jaccard overlaps between Hindi in Devanagari and its Latin transliteration are extremely high at all layers (typically $\gtrsim 0.9$), and overlaps with Devanagari ``noise'' text are slightly lower but still high. Most Hindi features that fire on natural text also fire on transliteration and related Indic languages, so we classify them as script-robust semantic or grammatical features. A much smaller subset of features fire on Devanagari noise or only on one script variant; these are our script-sensitive detectors.
  \item \emph{Hierarchy summary.} Aggregating across layers, early layers are dominated by shared and script-agnostic features (orthography and tokenization), mid layers retain high cross-lingual overlap while introducing more family-specific structure, and late layers add a thicker shell of language- and script-specific detectors on top of the shared core.
\end{itemize}

\subsection{Exp2, Exp7, Exp9, Exp10: Steering Methods and Layers}

\textbf{Goal.} Compare steering methods (dense, activation-diff SAE, monolinguality SAE, random, attribution-based) across layers and languages.

\textbf{Method.}
\begin{itemize}
  \item \textbf{Exp2:} Compare methods at selected layers (e.g., 5, 13, 20, 24) for EN$\to$HI steering.
  \item \textbf{Exp7:} Probe individual SAE features by steering them up/down and measuring changes in script ratio, semantic similarity, and degradation.
  \item \textbf{Exp9:} Full layer $\times$ method $\times$ language sweep (HI, BN, TA, TE, UR, DE, AR), evaluating multiple strengths and aggregating script, semantics, degradation, and LLM-judge scores.
  \item \textbf{Exp10:} Use occlusion attribution to identify causally important features and build an attribution-based steering vector, comparing it to dense and activation-diff vectors.
\end{itemize}

\textbf{Results (partial, Exp2).} In an initial EN$\to$HI steering run on FLORES prompts:
\begin{itemize}
  \item Dense and activation-difference SAE steering achieve very high Hindi script success (often $90$--$100\%$ of generations in Devanagari) at mid and late layers (e.g., 5, 13, 20) for suitable strengths.
  \item Monolinguality-selected feature sets and random feature baselines produce essentially $0\%$ language switching at all tested layers and strengths, consistent with their role as detectors and a negative control respectively.
  \item All methods, including dense and activation-diff, fail to steer at the very top SAE layer (24): EN$\to$HI success remains near $0\%$ regardless of strength. This supports a ``generator window'' picture in which effective steering directions live in mid--late layers, while the final layer is too close to logits for simple additive interventions to work.
\end{itemize}
Full cross-language sweeps (Exp9), attribution-based steering (Exp10), and single-feature causal probing (Exp7) are left as \textbf{N/A} until the corresponding experiments are fully re-run and summarized.

\subsection{Exp4: Spillover Analysis}

\textbf{Goal.} Measure spillover: how EN$\to$Hindi steering affects other Indic languages vs non-Indic controls.

\textbf{Method.} Apply a Hindi steering vector and classify outputs by script/language, measuring probabilities of outputs in HI, UR, BN, TA, TE, DE, AR.

\textbf{Results (placeholder).}
\begin{itemize}
  \item Spillover rates to each language under Hindi steering: \textbf{N/A}.
\end{itemize}

\subsection{Exp8: Scaling to 9B and Low-Resource Languages}

\textbf{Goal.} Compare 2B vs 9B representations and simple EN$\to$HI steering, including low-resource Indic and non-Indic languages.

\textbf{Method.} Compute active feature counts per language and layer, and evaluate dense vs SAE steering on a small set of prompts for EN$\to$HI.

\textbf{Results (placeholder).}
\begin{itemize}
  \item Feature coverage for 2B vs 9B per language and layer: \textbf{N/A}.
  \item EN$\to$HI success and degradation for 2B vs 9B layers: \textbf{N/A}.
\end{itemize}

\subsection{Exp11: Calibrated LLM-as-Judge}

\textbf{Goal.} Calibrate a Gemini-based judge for multilingual steering evaluation.

\textbf{Method.} Use a calibration set with ground-truth labels; estimate sensitivity, specificity, and bias-corrected accuracy with CIs as in recent LLM-as-judge analyses~\cite{krumdick2025nofreelabels}.

\textbf{Results (placeholder).}
\begin{itemize}
  \item Raw judge accuracy per language: \textbf{N/A}.
  \item Bias-corrected accuracy and 95\% CI per language: \textbf{N/A}.
\end{itemize}

\subsection{Exp12: QA Degradation Under Steering}

\textbf{Goal.} Quantify how EN$\to$target steering affects QA performance on MLQA and IndicQA.

\textbf{Method.} For each target language (HI, DE, AR; HI, BN, TA, TE), we:
\begin{itemize}
  \item Use Exp9 results to pick best steering layer, strength, and method.
  \item Generate baseline and steered answers for MLQA/IndicQA questions.
  \item Evaluate script dominance, LaBSE similarity to gold answers, degradation, and LLM-judge scores.
\end{itemize}

\textbf{Results (placeholder).}
\begin{itemize}
  \item MLQA/IndicQA baseline vs steered script success, semantic similarity, and degradation per language: \textbf{N/A}.
\end{itemize}

\section{Discussion}

Our framework is designed to support a full, data-driven account of Indic language representations and steering in Gemma models using SAEs. A subset of experiments (Exp1--3, Exp5--6, and an initial run of Exp2) has already produced stable qualitative patterns, while others (Exp4, Exp7--Exp16) are still in progress. We briefly summarize the emerging picture and then outline what the remaining experiments will add.

\paragraph{Detectors vs generators across depth.}
Exp1 shows that monolinguality-based \emph{detectors}---features that fire much more often for one language than others---are plentiful at all probed layers but become most numerous in late layers near the logits. Exp2, by contrast, reveals that effective EN$\to$HI \emph{generator} directions live in a mid--late ``window'' (e.g., layers 13 and 20) where dense and activation-difference steering reliably shift the model into Hindi, while interventions at the very top layer fail to change the output language at all. Together, these results support a decomposition in which
\begin{itemize}
  \item early and mid layers contain a mostly shared, language-agnostic concept space with some emerging family structure;
  \item late layers accumulate language- and script-specific detectors that report which language is being decoded; and
  \item the most useful steering vectors are built from features that are causally generative (activation-diff or attribution-based), not merely highly monolingual detectors.
\end{itemize}

\paragraph{Hindi--Urdu as one language with two scripts.}
Exp3 and the hierarchical analyses in Exp5--Exp6 consistently show that Hindi and Urdu share more SAE features than Hindi and English do at every layer. The Jaccard overlap between HI and UR active features remains very high (on the order of $0.93$--$0.99$ across layers) and always exceeds the HI--EN overlap. Decomposing the HI--UR union into semantic vs script-specific components, we find that most features are script-robust (also active for English), while only a small minority are script-sensitive and unique to one script. This supports a view in which Gemma encodes Hindi and Urdu as a shared semantic language subspace with a thin script shell on top.

\paragraph{Hierarchical structure and Indic clustering.}
Exp5 and Exp6 further refine this picture. Across all layers there is a large shared feature backbone that is active for all languages, suggesting a stable multilingual concept space. Within that backbone, the Indic cluster (HI, UR, BN, TA, TE) shares additional features that are rarely used by German or Arabic, and Hindi remains closest to Urdu even when we restrict attention to Indic-only features. Script-sensitive detectors are present but form a small fraction of the overall Indic feature mass, and Hindi in Devanagari has almost the same active feature set as its Latin transliteration. Conceptually, this yields a three-stage story: early layers dominated by orthography and tokenization, mid layers by shared semantics and family-level structure, and late layers by language-specific decoding and script detectors.

Once the remaining experiments (Exp4, Exp7--Exp16) are re-run and summarized, we expect the framework to support:

\begin{itemize}
  \item A hierarchical picture of where script vs semantic features and cross-lingual overlaps live.
  \item A detailed comparison of steering methods and layers across the Indic cluster and controls.
  \item Quantified spillover and leakage when steering a single Indic language.
  \item A systematic assessment of steering-induced degradation, both in free-form generation and QA.
\end{itemize}

Many numerical entries in the Results section remain \textbf{N/A}; the next step is to execute the full pipeline (e.g., \texttt{python run.py --exp4 --exp8 --exp9 --exp10 --exp14 --exp15 --exp16} followed by \texttt{python summarize\_results.py}) and populate the pending tables and figures, especially for spillover, attribution-based steering, cross-layer alignment, directional symmetry, and robustness on QA and code-mixed inputs.

\section{Limitations and Future Work}

\begin{itemize}
  \item \textbf{No human-annotated evaluation yet.} We rely on structural metrics, LaBSE, and a calibrated Gemini judge. Human evaluation is needed to validate conclusions.
  \item \textbf{Single architecture.} We focus on Gemma 2 (2B, 9B). Cross-architecture comparison (e.g., LLaMA, Qwen) is left for future work.
  \item \textbf{Single steering schedule.} We use constant-strength steering. More sophisticated schedules (e.g., decaying or token-aware) may reduce degradation.
  \item \textbf{Partial circuit view.} While we probe individual features and do occlusion attribution at single layers, we do not yet reconstruct full multi-layer circuits.
\end{itemize}

\section{Conclusion}

We have presented a unified, research-grade experimental framework for studying Indic language representations and steering in Gemma models using SAEs, covering hierarchy, steering methods and layers, spillover, and robustness. Once populated with data, this framework will support a detailed preprint on how Indic languages are encoded and controlled in Gemma, and how steering trades off control against degradation.

\appendix
\section{Algorithms for Feature Statistics}
\label{app:stats}
This appendix sketches the core routines used throughout the experiments.
The codebase implements them in \texttt{data.py}, \texttt{model.py}, and
\texttt{experiments/*.py}; we include high-level pseudocode here for clarity.

\subsection{Activation Rates and Monolinguality}
\begin{verbatim}
def compute_activation_rates(model, sae, texts, layer):
    counts = np.zeros(num_features)
    total_tokens = 0
    for text in texts:
        h = model.get_hidden_states(text, layer)   # [T, d]
        z = sae.encode(h)                          # [T, m]
        active = (z > 0).any(axis=0)               # [m]
        counts += active.astype(int)
        total_tokens += len(text_tokens(text))
    return counts / total_tokens

def monolinguality(P_target, P_others, eps=1e-4):
    M = np.zeros_like(P_target)
    mask = P_target > eps
    M[mask] = P_target[mask] / np.maximum(P_others[mask], eps)
    return M
\end{verbatim}

\subsection{Steering Vector Construction}
\begin{verbatim}
def dense_steering(model, texts_src, texts_tgt, layer):
    h_src = mean_hidden(model, texts_src, layer)   # [d]
    h_tgt = mean_hidden(model, texts_tgt, layer)   # [d]
    v = h_tgt - h_src
    return v / np.linalg.norm(v)

def sae_activation_diff_steering(model, sae, texts_src, texts_tgt,
                                 layer, k):
    P_src  = activation_rates(model, sae, texts_src, layer)
    P_tgt  = activation_rates(model, sae, texts_tgt, layer)
    diff   = P_tgt - P_src
    topk   = np.argsort(diff)[-k:]
    W_dec  = sae.decoder_weights()                 # [d, m]
    v      = W_dec[:, topk].mean(axis=1)
    return v / np.linalg.norm(v)
\end{verbatim}

\section{Additional Implementation Notes}
\label{app:impl}
All experiments are run on NVIDIA A100 40GB GPUs with Gemma models loaded
in bfloat16. SAEs from Gemma Scope are attached to the residual stream and
their decoded steering vectors are always cast back to the model's hidden
state dtype to avoid mixed-precision errors. Dataset splits are constructed
to avoid train--test leakage by removing any FLORES sentences that appear
in both training and evaluation sets.

\bibliographystyle{plain}
\bibliography{references}

\end{document}
