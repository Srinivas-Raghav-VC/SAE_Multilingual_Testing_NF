% =============================================================================
% Indic Language Representations and Steering in Gemma Models with SAEs
% Conference-quality formatting (ACL/EMNLP/NeurIPS style)
% =============================================================================

\documentclass[11pt,a4paper]{article}

% =============================================================================
% PACKAGES
% =============================================================================

% Page geometry - conference style
\usepackage[margin=2.5cm]{geometry}

% Typography
\usepackage[T1]{fontenc}
\usepackage{mathptmx}           % Times font for text and math
\usepackage{microtype}          % Better typography
\usepackage{setspace}           % Line spacing
\usepackage{xspace}             % Smart spacing for macros

% Graphics and figures
\usepackage{graphicx}
\usepackage{float}
\usepackage{wrapfig}
\usepackage[font=small,labelfont=bf,labelsep=period]{caption}
\usepackage{subcaption}

% Tables
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{tabularx}

% Math
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{bm}                 % Bold math

% Colors
\usepackage[dvipsnames]{xcolor}
\definecolor{linkblue}{RGB}{0,51,153}
\definecolor{citegreen}{RGB}{0,102,51}

% Algorithms
\usepackage{algorithm}
\usepackage{algorithmic}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

% Lists
\usepackage{enumitem}
\setlist{nosep,leftmargin=*}

% Citations
\usepackage{natbib}

% Hyperlinks (load last among these)
\usepackage[breaklinks=true]{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=linkblue,
    citecolor=citegreen,
    urlcolor=linkblue,
    pdftitle={Indic Language Representations and Steering in Gemma Models with Sparse Autoencoders},
    pdfauthor={Srinivas Raghav V C},
}
\usepackage[capitalise,noabbrev]{cleveref}

% =============================================================================
% CUSTOM COMMANDS
% =============================================================================

\newcommand{\ie}{i.e.,\xspace}
\newcommand{\eg}{e.g.,\xspace}
\newcommand{\cf}{cf.\xspace}
\newcommand{\wrt}{w.r.t.\xspace}
\newcommand{\etal}{\textit{et al}.\xspace}

% Math shortcuts
\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\loss}{\mathcal{L}}
\newcommand{\data}{\mathcal{D}}
\newcommand{\feat}{\mathcal{F}}

% Model/method names
\newcommand{\gemma}{\textsc{Gemma}}
\newcommand{\sae}{\textsc{SAE}}
\newcommand{\flores}{\textsc{Flores-200}}

% Highlight boxes for key equations
\usepackage{tcolorbox}
\tcbuselibrary{skins,breakable}
\newtcolorbox{keyeq}{
    colback=gray!5,
    colframe=gray!50,
    boxrule=0.5pt,
    arc=2pt,
    left=4pt,right=4pt,top=2pt,bottom=2pt,
    before skip=6pt,after skip=6pt
}

% =============================================================================
% TITLE AND AUTHORS
% =============================================================================

\title{\LARGE\bfseries Indic Language Representations and Steering\\in Gemma Models with Sparse Autoencoders}

\author{
    \textbf{Srinivas Raghav V C}\\
    Indian Institute of Information Technology Kottayam\\
    \texttt{srinivas22bcs16@iiitkottayam.ac.in}
}

\date{}

% =============================================================================
% DOCUMENT
% =============================================================================

\begin{document}

\maketitle
\thispagestyle{empty}

% -----------------------------------------------------------------------------
% ABSTRACT
% -----------------------------------------------------------------------------

\begin{abstract}
\noindent
Multilingual large language models are increasingly deployed in settings where language and script control is critical, yet their internal multilingual representations remain poorly understood---particularly for low-resource language families like Indic languages. We study Indic language representations (Hindi, Urdu, Bengali, Tamil, Telugu) in Gemma~2 using Sparse Autoencoders (SAEs) from Gemma Scope. Across selected layers and thousands of sentences per language (Samanantar; with FLORES for additional controls), we find: \textbf{(1)}~language-selective \emph{detector} features exhibit strongly language-dependent depth profiles, including late-layer spikes consistent with output-stage script disambiguation; \textbf{(2)}~Hindi--Urdu active feature sets show high overlap together with a small script-specific shell, supporting a shared semantic subspace with orthographic specialization; \textbf{(3)}~in steering sweeps, dense and activation-difference directions are most effective in mid-to-late layers, while many highly selective features behave as detectors rather than generators; causal occlusion identifies a validated subset of generator-relevant features. We provide a reproducible evaluation protocol with bootstrap confidence intervals, paired statistical tests, and calibrated LLM-as-judge scores.
\end{abstract}

\vspace{0.5em}
\noindent\textbf{Keywords:} sparse autoencoders, multilingual representations, Indic languages, mechanistic interpretability, activation steering

\vspace{1em}

% -----------------------------------------------------------------------------
% 1. INTRODUCTION
% -----------------------------------------------------------------------------

\section{Introduction}
\label{sec:intro}

Multilingual large language models (LLMs) are increasingly deployed in linguistically diverse societies where precise control over output language and script is essential. This is particularly critical in the Indian subcontinent, where over 1.4 billion people use languages spanning multiple scripts---from Devanagari (Hindi, Marathi) to Arabic-derived Nastaliq (Urdu) to distinct scripts for Dravidian languages (Tamil, Telugu, Kannada, Malayalam). Understanding how models internally represent these languages, and how to steer generation toward specific languages without catastrophic degradation, is crucial for both interpretability research and practical deployment.

Sparse Autoencoders (SAEs) have emerged as a powerful lens for interpreting LLM representations \citep{bricken2023monosemanticity,cunningham2023sparse,gemmascope2024}. By decomposing residual stream activations into overcomplete, sparse feature dictionaries, SAEs expose interpretable features that individual neurons obscure due to polysemanticity. Prior work has shown that some SAE features behave as \emph{detectors} (correlating with specific inputs) while others act as \emph{generators} (causally influencing outputs when manipulated) \citep{templeton2024scaling,marks2024sparse}. However, systematic application of SAEs to multilingual representations---particularly for the understudied Indic language family---remains limited.

We present a comprehensive study of Indic language representations and steering in Gemma~2 (2B and 9B parameters) using Gemma Scope SAEs \citep{gemmascope2024}. Our work makes three primary contributions:

\begin{enumerate}[leftmargin=2em]
    \item \textbf{Asymmetric language encoding across depth.} We find that languages have distinct detector profiles across depth, with some languages exhibiting late-layer spikes consistent with script-level disambiguation. Hindi--Urdu show high overlap in active feature sets together with a thin script-specific shell.

    \item \textbf{Detector vs.\ generator distinction.} We compare steering approaches (dense, activation-difference, monolinguality-based, and attribution-based) across layers. Monolinguality-based selectors often identify detectors that are predictive but weak for generation, while mid-to-late layer directions provide a ``generator window.'' We support this distinction with causal occlusion analyses, including logit-level controls.

    \item \textbf{Rigorous evaluation methodology.} We quantify steering success and degradation using structural metrics, LaBSE semantic similarity, and a calibrated LLM-as-judge \citep{krumdick2025nofreelabels}, with bootstrap confidence intervals (10,000 resamples) and paired statistical tests throughout.
\end{enumerate}

Our framework comprises a suite of experiments spanning feature discovery, steering comparison, spillover analysis, scaling, and downstream QA evaluation. We release our codebase to support reproducible research on multilingual representations.

% -----------------------------------------------------------------------------
% 2. RELATED WORK
% -----------------------------------------------------------------------------

\section{Related Work}
\label{sec:related}

\paragraph{Multilingual representations in transformers.}
A substantial body of work has examined how multilingual transformers develop cross-lingual representations. Early studies on mBERT demonstrated surprising zero-shot cross-lingual transfer \citep{pires2019multilingual,wu2019beto}, with subsequent work revealing that such models learn language-neutral representations in intermediate layers \citep{conneau2020emerging,libovicky2020language}. \citet{chi2020finding} showed that mBERT encodes universal grammatical relations, while \citet{rajaee2022isotropy} analyzed isotropy in multilingual embedding spaces. More recent work on decoder-only LLMs finds that models like LLaMA process multilingual inputs through a shared ``concept space'' with language-specific decoding in later layers \citep{wendler2024llamas}. \citet{doddapaneni2021primer} provide a comprehensive survey. Our work extends this line by using SAEs to decompose Gemma's residual streams into sparse features, explicitly separating script-specific from semantic components.

\paragraph{Indic language resources and modeling.}
The Indic NLP landscape has matured significantly with resources such as IndicNLPSuite \citep{kakwani2020indicnlpsuite}, MuRIL \citep{khanuja2021muril}, and Samanantar \citep{ramesh2021samanantar}---the largest publicly available EN--Indic parallel corpus spanning 11 languages. \flores{} \citep{goyal2021flores} provides high-quality parallel evaluation data. Linguistic studies on Hindi--Urdu consistently suggest shared underlying grammar and vocabulary with differences primarily in script, making them an ideal test case for disentangling script from semantics.

\paragraph{Sparse Autoencoders and mechanistic interpretability.}
SAEs have emerged as a powerful tool for interpreting LLM activations \citep{bricken2023monosemanticity,cunningham2023sparse}. This approach addresses the superposition hypothesis \citep{elhage2022toymodels}, which proposes that networks store more features than dimensions via overlapping directions. Anthropic's work on Claude \citep{templeton2024scaling} and Google's Gemma Scope \citep{gemmascope2024} have scaled SAEs to frontier models, enabling circuit-level analysis \citep{conmy2023automated,marks2024sparse,wang2022interpretability}. \citet{oneill2024sparse} demonstrated reliable circuit identification with SAEs.

\paragraph{Activation steering and representation engineering.}
Activation addition \citep{turner2023activation} and representation engineering \citep{zou2023representation} show that editing activations along carefully chosen directions can systematically alter model behavior. Related work on inference-time intervention \citep{li2024inference} and latent knowledge discovery \citep{burns2022discovering} demonstrates the power of targeted modifications. \citet{zhang2024towards} provide best practices for activation patching, while \citet{geva2023dissecting} and \citet{stolfo2023mechanistic} use causal mediation analysis. We extend these ideas to Indic language control.

\noindent Recent concurrent work studies \emph{causal language control} via sparse feature steering in multilingual transformers \citep{chou2025causal}, and highlights that correlation-based feature selection can be vulnerable to spurious steering artifacts, proposing correlation-aware controls \citep{cho2025corrsteer}. Our contribution is complementary: we focus on the Indic cluster (including Hindi--Urdu script/semantic disentanglement), spillover structure, and robustness/QA impact with a calibrated judge stack.

\paragraph{LLM-as-judge and calibration.}
LLM-as-judge methods show strong correlation with human judgments but also significant biases \citep{krumdick2025nofreelabels,dorner2024limits}. We adopt a calibration approach that estimates bias-corrected accuracy with confidence intervals.

% -----------------------------------------------------------------------------
% 3. BACKGROUND AND HYPOTHESES
% -----------------------------------------------------------------------------

\section{Background and Hypotheses}
\label{sec:background}

We consider a transformer LLM with $L$ layers and hidden dimension $d$, augmented with SAEs trained on residual stream activations. Each SAE encodes a hidden state $h \in \R^d$ into sparse features $z \in \R^m$ and reconstructs $\hat{h} \in \R^d$.

\subsection{Formalization}

Let $\loss$ denote the set of languages we study (EN, HI, UR, BN, TA, TE, DE, AR). For an input token sequence $x_{1:T}$ and layer $\ell$, we write $h_{\ell,t}(x) \in \R^d$ for the residual stream activation at position $t$.

For each layer $\ell$ we attach a sparse autoencoder $(f_\ell, g_\ell)$ with feature dimension $m \gg d$:
\begin{align}
    z_{\ell,t} &= f_\ell(h_{\ell,t}) \in \R^m, \\
    \hat{h}_{\ell,t} &= g_\ell(z_{\ell,t}) \in \R^d,
\end{align}
where $z_{\ell,t}$ is sparse and $\hat{h}_{\ell,t}$ reconstructs $h_{\ell,t}$. We denote the decoder matrix by $W^{(\ell)}_{\text{dec}} \in \R^{m \times d}$.

For feature $j$ and language $L$, we estimate the \emph{activation rate}:
\begin{keyeq}
\begin{equation}
    \hat{P}_\ell(j \mid L) = \frac{\sum_{x \in \data_L} \sum_{t} \mathbf{1}[z_{\ell,t,j} > 0]}{\sum_{x \in \data_L} |x|}
    \label{eq:activation-rate}
\end{equation}
\end{keyeq}

We define \emph{monolinguality} for feature $(\ell,j)$ and target language $T$:
\begin{keyeq}
\begin{equation}
    M_{\ell,j}(T) = \frac{\hat{P}_\ell(j \mid T)}{\max_{L \neq T} \hat{P}_\ell(j \mid L)}
    \label{eq:monolinguality}
\end{equation}
\end{keyeq}
with features below activation threshold $\epsilon$ set to zero. A feature is \emph{strongly $T$-selective} if $M_{\ell,j}(T) > 3$.

For cross-language overlap, we compute \emph{Jaccard similarity}:
\begin{keyeq}
\begin{equation}
    J_\ell(L_1, L_2) = \frac{|\feat_\ell(L_1) \cap \feat_\ell(L_2)|}{|\feat_\ell(L_1) \cup \feat_\ell(L_2)|}
    \label{eq:jaccard}
\end{equation}
\end{keyeq}
where $\feat_\ell(L) = \{j : \hat{P}_\ell(j \mid L) > \tau\}$.

\paragraph{Steering.} We intervene on hidden states by adding steering vector $v_\ell$:
\begin{keyeq}
\begin{equation}
    \tilde{h}_{\ell,t} = h_{\ell,t} + \alpha v_\ell
    \label{eq:steering}
\end{equation}
\end{keyeq}

For \emph{dense} steering: $v_\ell = \E_{x \in \data_T}[\bar{h}_\ell(x)] - \E_{x \in \data_S}[\bar{h}_\ell(x)]$.

For \emph{SAE-based} steering: $v_\ell = \frac{1}{|S_\ell|} \sum_{j \in S_\ell} W^{(\ell)}_{\text{dec}}[j,:]$ where $S_\ell$ is a selected feature set.

\paragraph{Detector vs.\ generator features.}
\emph{Detectors} have high monolinguality $M_{\ell,j}(T)$---they fire when language $T$ is present but don't causally influence output. \emph{Generators} have high causal effect: steering along their decoder direction changes output language. This distinction explains why high-monolinguality features can be poor for steering.

\subsection{Hypotheses}

\noindent\textbf{T1: Hierarchical Geometry}
\begin{itemize}
    \item \textbf{H1:} Hindi-specific detector features ($M > 3$) exist at multiple layers.
    \item \textbf{H2:} Shared cross-lingual features peak in mid-layers.
    \item \textbf{H3:} Hindi--Urdu overlap exceeds Hindi--English at all layers.
    \item \textbf{H4:} Script-specific features form ${<}20\%$ of Hindi--Urdu union.
\end{itemize}

\noindent\textbf{T2: Steering Methods}
\begin{itemize}
    \item \textbf{H5:} Activation-difference steering outperforms monolinguality-based.
    \item \textbf{H6:} Optimal steering layers exist (mid-to-late window).
    \item \textbf{H7:} Attribution-based steering may improve over activation-diff.
\end{itemize}

\noindent\textbf{T3: Spillover}
\begin{itemize}
    \item \textbf{H8:} EN$\to$Hindi steering affects Indic languages more than controls.
\end{itemize}

\noindent\textbf{T4: Robustness}
\begin{itemize}
    \item \textbf{H10:} Strong steering increases degradation.
    \item \textbf{H11:} Steering harms downstream QA performance.
\end{itemize}

% -----------------------------------------------------------------------------
% 4. EXPERIMENTAL SETUP
% -----------------------------------------------------------------------------

\section{Experimental Setup}
\label{sec:setup}

\subsection{Models and SAEs}

We use Gemma~2 2B ($L=26$ layers, $d=2304$) and optionally Gemma~2 9B ($L=42$ layers, $d=3584$) \citep{gemma22024}. SAEs from Gemma Scope \citep{gemmascope2024} are attached at multiple residual-stream layers, each with $m=16384$ features. For 2B we probe layers $\{5, 8, 10, 13, 16, 20, 24\}$; for 9B we probe relative-depth matched layers $\{8, 14, 21, 28, 35, 40\}$.

\subsection{Datasets}

\paragraph{Samanantar} \citep{ramesh2021samanantar}: EN--Indic parallel corpus for feature discovery ($N=5000$ samples per language).

\paragraph{\flores{}} \citep{goyal2021flores}: High-quality parallel data for cross-language analysis and evaluation prompts ($N=200$ for steering). We use FLORES for (i) parallel overlap/alignment analyses and (ii) held-out English prompts for steering evaluation. When FLORES is used for both steering-vector estimation and evaluation in a given experiment, we use disjoint splits (\texttt{dev} for estimation; \texttt{devtest} for evaluation) and remove any exact overlaps, so the evaluation prompts are not re-used in vector construction (see \Cref{sec:limitations}).

\paragraph{MLQA/IndicQA}: Downstream QA evaluation \citep{lewis2019mlqa}. (Our code uses a robust HuggingFace loader with fallbacks when dataset scripts are unavailable under modern \texttt{datasets} versions; exact sources are logged in the experiment outputs.)

\subsection{Evaluation Metrics}

\begin{itemize}
    \item \textbf{Script detection:} Unicode-based script ratios with dominance thresholds.
    \item \textbf{Semantic similarity:} LaBSE \citep{feng2020labse} cosine similarity. For steering prompts we report prompt-faithfulness (LaBSE(prompt, output)); optionally, baseline-preservation (LaBSE(baseline, steered)). For QA, prompt-vs-answer similarity is not meaningful for long contexts, so we report baseline-preservation (LaBSE(baseline, steered)) as a minimal-change proxy, and report answer quality via EM/F1 against gold answers. We report LaBSE truncation rates.
    \item \textbf{Degradation:} 3-gram and 5-gram repetition ratios.
    \item \textbf{Language ID (controls):} For shared-script controls (Latin/Arabic), we optionally use a fastText-based LID backend; otherwise we interpret controls as script-dominance effects.
    \item \textbf{LLM judge:} Calibrated Gemini judge with bias-corrected accuracy \citep{krumdick2025nofreelabels}.
\end{itemize}

\noindent\textbf{Completion-only evaluation.} For all generation-based metrics, we evaluate only the \emph{generated completion} (prompt tokens removed) to avoid confounding the prompt language and context with the steered output, especially in long-context QA prompts.

\paragraph{Statistical testing.} All metrics include 95\% bootstrap CIs (10,000 resamples). Method comparisons use paired Wilcoxon tests with Holm-Bonferroni correction; we report signed rank-biserial effect sizes for Wilcoxon (and Cohen's $d$ where applicable).

% -----------------------------------------------------------------------------
% FIGURE 1: Architecture (conceptual)
% -----------------------------------------------------------------------------

\begin{figure}[t]
    \centering
    \IfFileExists{figures/architecture_multilingual_sae.pdf}{
        \includegraphics[width=\textwidth]{figures/architecture_multilingual_sae}
    }{
        \fbox{\parbox[c][0.22\textheight][c]{0.9\textwidth}{\centering Placeholder for architecture diagram\\(save as \texttt{figures/architecture\_multilingual\_sae.pdf})}}
    }
    \caption{\textbf{Gemma-2 with Gemma Scope SAEs (conceptual).} \textit{Left:} Multilingual inputs in various scripts (EN, HI, UR, BN, TA, TE, DE, AR). \textit{Middle:} The transformer with SAEs attached at selected residual stream layers. Orange layers indicate target layers for analysis. \textit{Right:} Each SAE encodes residual activations into 16k sparse features capturing language, script, syntax, and semantic information, from which we construct steering vectors.}
    \label{fig:architecture}
\end{figure}

% -----------------------------------------------------------------------------
% FIGURE 2: Overview (conceptual)
% -----------------------------------------------------------------------------

\begin{figure}[t]
    \centering
    \IfFileExists{figures/overview_comic.pdf}{
        \includegraphics[width=\textwidth]{figures/overview_comic}
    }{
        \fbox{\parbox[c][0.22\textheight][c]{0.9\textwidth}{\centering Placeholder for overview comic\\(save as \texttt{figures/overview\_comic.pdf})}}
    }
    \caption{\textbf{Overview: Steering Gemma's multilingual representations (conceptual).} \textit{Panel 1:} Multilingual prompts in English and Indic languages. \textit{Panel 2:} Gemma-2 with SAEs at multiple layers. \textit{Panel 3:} Varying steering strength $\alpha$ shifts generation from English to Indic outputs. \textit{Panel 4:} We evaluate using script ratios, LaBSE similarity, degradation metrics, and a calibrated Gemini judge.}
    \label{fig:overview}
\end{figure}

% -----------------------------------------------------------------------------
% FIGURE 3: Evaluation pipeline (conceptual)
% -----------------------------------------------------------------------------

\begin{figure}[t]
    \centering
    \IfFileExists{figures/steering_eval_pipeline.pdf}{
        \includegraphics[width=\textwidth]{figures/steering_eval_pipeline}
    }{
        \fbox{\parbox[c][0.22\textheight][c]{0.9\textwidth}{\centering Placeholder for evaluation pipeline diagram\\(save as \texttt{figures/steering\_eval\_pipeline.pdf})}}
    }
    \caption{\textbf{Steering, evaluation, and calibrated judge pipeline.} We apply steering vectors (dense or SAE-based) at a chosen layer and strength, generate completions, and evaluate (i) target-script dominance, (ii) semantic similarity via LaBSE, and (iii) degradation via repetition statistics. Gemini LLM-as-judge scores are cached and rate-limited; Exp11 estimates judge sensitivity/specificity and reports bias-corrected accuracies with confidence intervals.}
    \label{fig:eval-pipeline}
\end{figure}

% -----------------------------------------------------------------------------
% 5. EXPERIMENTS AND RESULTS
% -----------------------------------------------------------------------------

\section{Experiments}
\label{sec:experiments}

\subsection{Geometry of Indic Representations}

\paragraph{Exp1: Feature Discovery.}
We compute monolinguality scores $M_{\ell,j}(T)$ for all 16,384 SAE features across seven layers, using 5,000 sentences per language from Samanantar (HI, BN, TA, TE, EN) and approximately 1k sentences from FLORES-200 \emph{dev} for additional languages (UR, DE, AR). A feature is \emph{strongly selective} for language $L$ if $M_{\ell,j}(L) > 3$, meaning it fires at least 3$\times$ more often for $L$ than any other language.

\Cref{tab:feature-discovery} presents the complete results. We test two hypotheses:

\begin{itemize}[leftmargin=2em]
    \item \textbf{H1:} Each probed layer contains $\geq$10 Hindi-selective detector features. \textbf{Result: PASS} in our probed layers (see \Cref{tab:feature-discovery}).
    \item \textbf{H3:} Detector features peak in mid-layers (40--60\% of depth). \textbf{Result: not supported} in our probed layers; detector density trends later than the mid-range (see \Cref{tab:feature-discovery,fig:exp1-selectivity}).
\end{itemize}

\IfFileExists{results/tables/tab_exp1_feature_discovery.tex}{
    \input{results/tables/tab_exp1_feature_discovery}
}{
    \begin{table}[t]
    \centering
    \caption{\textbf{Language-specific detector features per layer} (auto-generated from \texttt{results/exp1\_feature\_discovery.json}).}
    \label{tab:feature-discovery}
    \fbox{\parbox[c][0.12\textheight][c]{0.9\textwidth}{\centering Placeholder for auto-generated table\\(run \texttt{python3 summarize\_results.py})}}
    \end{table}
}

% -----------------------------------------------------------------------------
% Empirical plot: Exp1 detector counts (auto-generated by plots.py)
% -----------------------------------------------------------------------------

\begin{figure}[t]
    \centering
    \IfFileExists{results/figures/fig1_exp1_selectivity_heatmap.png}{
        \includegraphics[width=0.95\textwidth]{results/figures/fig1_exp1_selectivity_heatmap}
    }{
        \fbox{\parbox[c][0.18\textheight][c]{0.9\textwidth}{\centering Placeholder for auto-generated plot\\(run \texttt{python3 plots.py --results\_dir results})}}
    }
    \caption{\textbf{Empirical: language-selective detector counts across depth (Exp1).} Heatmap shows, for each language and probed layer, the number of SAE features with monolinguality $M>3$.}
    \label{fig:exp1-selectivity}
\end{figure}

\paragraph{Key observations.} The data reveals striking asymmetries across languages:

\begin{enumerate}[leftmargin=2em]
    \item \textbf{Depth profiles differ by language:} detector counts can peak early for some languages and rise sharply late for others, indicating language identity signals are not uniform across depth.
    \item \textbf{Late-layer spikes exist:} some languages exhibit late-layer increases consistent with output-stage disambiguation (e.g., script-level effects) rather than purely semantic divergence.
    \item \textbf{Indic vs non-Indic asymmetries:} Indic languages and Latin-script controls can show distinct selectivity profiles, consistent with a shared core representation plus language-/script-specific shells.
    \item \textbf{Interpretation is operational:} these are activation-defined ``detectors''; causal claims require intervention experiments (Exp10/13).
\end{enumerate}

The H3 result is informative: in our probed layers, activation-defined \emph{detector} features tend to concentrate later than the mid-range. This motivates separating correlational detectors from causally useful generator directions (validated in Exp10/13).

\paragraph{Exp3: Hindi--Urdu Overlap.}
We compute Jaccard overlap between active feature sets to test whether Hindi and Urdu are encoded as one language with two scripts. \Cref{tab:hindi-urdu-overlap} presents the results.

\IfFileExists{results/tables/tab_exp3_hi_ur_overlap.tex}{
    \input{results/tables/tab_exp3_hi_ur_overlap}
}{
    \begin{table}[t]
    \centering
    \caption{\textbf{Hindi--Urdu feature overlap across layers} (auto-generated from \texttt{results/exp3\_hindi\_urdu\_fixed.json}).}
    \label{tab:hindi-urdu-overlap}
    \fbox{\parbox[c][0.12\textheight][c]{0.9\textwidth}{\centering Placeholder for auto-generated table\\(run \texttt{python3 summarize\_results.py})}}
    \end{table}
}

\textbf{Hypothesis tests:}
\begin{itemize}[leftmargin=2em]
    \item \textbf{H4a:} Hindi--Urdu overlap is consistently high across probed layers (see \Cref{tab:hindi-urdu-overlap}).
    \item \textbf{H4b:} Script-specific features form a small shell relative to the shared set (see \Cref{tab:hindi-urdu-overlap}).
    \item \textbf{H4c:} Hindi--Urdu overlap exceeds Hindi--English across probed layers, and the gap persists under stricter activation thresholds (see Exp3 sensitivity analysis in the JSON output).
\end{itemize}

We treat these overlap measures as distributional evidence about representation geometry, not as proof of ``one language'' in a linguistic sense.

\paragraph{Exp5--6: Hierarchical Structure and Script Controls.}
We analyze the hierarchical organization of multilingual features (\Cref{tab:hierarchical}) and verify that script is separable from semantics using transliteration controls.

\IfFileExists{results/tables/tab_exp5_hierarchy.tex}{
    \input{results/tables/tab_exp5_hierarchy}
}{
    \begin{table}[t]
    \centering
    \caption{\textbf{Hierarchical feature organization} (auto-generated from \texttt{results/exp5\_hierarchical\_analysis.json}).}
    \label{tab:hierarchical}
    \fbox{\parbox[c][0.12\textheight][c]{0.9\textwidth}{\centering Placeholder for auto-generated table\\(run \texttt{python3 summarize\_results.py})}}
    \end{table}
}

\IfFileExists{results/tables/tab_exp6_script_semantics.tex}{
    \input{results/tables/tab_exp6_script_semantics}
}{
    \begin{table}[t]
    \centering
    \caption{\textbf{Script vs.\ semantics control (Exp6)} (auto-generated from \texttt{results/exp6\_script\_semantics\_controls.json}).}
    \label{tab:script-semantics}
    \fbox{\parbox[c][0.12\textheight][c]{0.9\textwidth}{\centering Placeholder for auto-generated table\\(run \texttt{python3 summarize\_results.py})}}
    \end{table}
}

\textbf{Key finding:} Hindi in Devanagari exhibits high overlap with its Latin transliteration across layers, supporting that most active features are driven by \emph{semantic content} rather than script. Script-only features form a comparatively thin orthographic shell atop shared representations (see \Cref{tab:script-semantics}).

% -----------------------------------------------------------------------------
% Empirical plot: Exp5 overlap clustermap (auto-generated by plots.py)
% -----------------------------------------------------------------------------

\begin{figure}[t]
    \centering
    \IfFileExists{results/figures/fig8_language_overlap_clustermap_middle.png}{
        \includegraphics[width=0.95\textwidth]{results/figures/fig8_language_overlap_clustermap_middle}
    }{
        \fbox{\parbox[c][0.18\textheight][c]{0.9\textwidth}{\centering Placeholder for auto-generated overlap clustermap\\(run \texttt{python3 plots.py --results\_dir results})}}
    }
    \caption{\textbf{Empirical: language overlap structure (Exp5).} Clustered heatmap shows pairwise active-feature overlap (Jaccard) at a representative mid-depth band.}
    \label{fig:exp5-overlap}
\end{figure}

\paragraph{Feature Interpretability.}
To validate that SAE features capture meaningful linguistic concepts, we used Gemini to label top-activating features at layer 20. Examples include:
\begin{itemize}[leftmargin=2em,topsep=2pt]
    \item \textbf{Feature 3899}: ``Legal/Business Declarations'' (Hindi, English)
    \item \textbf{Feature 10729}: ``Complex Sentences with Negation/Contrast'' (Hindi)
    \item \textbf{Feature 1701}: ``Attribution and Quotes'' (Urdu-specific)
    \item \textbf{Feature 6181}: ``Quotation Attribution in News'' (Hindi, Urdu)
\end{itemize}
These labels suggest that SAE features often capture interpretable semantic and syntactic patterns, not just statistical regularities.
We treat these LLM-proposed labels as qualitative illustrations rather than validated ground-truth annotations.

\subsection{Steering Methods and Layers}

\paragraph{Exp9: Layer $\times$ Method $\times$ Language Sweep.}
Our primary steering evidence comes from Exp9, which evaluates dense, activation-diff SAE, monolinguality SAE, and random baselines across target languages, layers, and strengths. Across our runs, dense and activation-diff directions are typically most effective in a mid-to-late ``generator window,'' while monolinguality-selected features and random baselines are generally weaker. Steering at the latest probed layers is often less effective, consistent with the model being closer to committing to a decoding pathway.

% -----------------------------------------------------------------------------
% Empirical plot: Exp9 steering heatmap (auto-generated by plots.py)
% -----------------------------------------------------------------------------

\begin{figure}[t]
    \centering
    \IfFileExists{results/figures/fig9_steering_heatmap_hi.png}{
        \includegraphics[width=0.95\textwidth]{results/figures/fig9_steering_heatmap_hi}
    }{
        \fbox{\parbox[c][0.18\textheight][c]{0.9\textwidth}{\centering Placeholder for auto-generated Exp9 heatmap\\(run \texttt{python3 plots.py --results\_dir results} after Exp9)}}
    }
    \caption{\textbf{Empirical: steering success by layer and method (Exp9; Hindi).} For each (layer, method), we plot the best success rate across strengths using script+semantic criteria (or script-only if semantics are unavailable).}
    \label{fig:exp9-heatmap-hi}
\end{figure}

\paragraph{Exp10: Attribution-Based Steering.}
Occlusion-based feature selection constructs attribution steering vectors. \textbf{Finding:} Attribution-based methods show modest improvements over activation-diff in some configurations, suggesting an avenue for refinement rather than a uniformly dominant method.

\subsection{Spillover and Degradation}

\paragraph{Exp4: Spillover.}
Exploratory spillover analysis compares EN$\to$HI steering to an EN$\to$DE control. We observe that EN$\to$HI steering tends to activate other Indic outputs (e.g., Urdu/Bengali) more than non-Indic controls, while EN$\to$DE does not spuriously increase Indic outputs. We treat this as suggestive evidence of an Indic cluster in steering space.

\begin{figure}[t]
    \centering
    \IfFileExists{results/figures/fig_exp4_spillover.png}{
        \includegraphics[width=0.95\textwidth]{results/figures/fig_exp4_spillover}
    }{
        \fbox{\parbox[c][0.18\textheight][c]{0.9\textwidth}{\centering Placeholder for spillover plot\\(run \texttt{python3 plots.py --results\_dir results} after Exp4)}}
    }
    \caption{\textbf{Empirical: spillover distributions (Exp4).} EN$\to$HI steering increases Indic outputs more than an EN$\to$DE control. For shared-script controls, language ID is best-effort (fastText recommended) and should be interpreted with caution for short outputs.}
    \label{fig:exp4-spillover}
\end{figure}

\paragraph{Exp12: QA Degradation.}
Exp12 evaluates downstream QA under steering on MLQA/IndicQA by comparing baseline answers to answers produced with a fixed steering configuration (selected from Exp9 when available). We report QA EM/F1 deltas and a baseline-preservation proxy LaBSE(baseline, steered) to quantify the ``minimal change'' trade-off. We treat QA impact as an empirical robustness check rather than a claim that any specific $\alpha$ threshold universally degrades performance.

\begin{figure}[t]
    \centering
    \IfFileExists{results/figures/fig_exp12_qa_delta_f1.png}{
        \includegraphics[width=0.95\textwidth]{results/figures/fig_exp12_qa_delta_f1}
    }{
        \fbox{\parbox[c][0.18\textheight][c]{0.9\textwidth}{\centering Placeholder for QA impact plot\\(run \texttt{python3 plots.py --results\_dir results} after Exp12)}}
    }
    \caption{\textbf{Empirical: QA impact under steering (Exp12).} Bars show $\Delta$F1 (steered -- baseline) for each QA task/language, enabling an explicit robustness check of downstream performance degradation.}
    \label{fig:exp12-qa}
\end{figure}

% -----------------------------------------------------------------------------
% FIGURE 4: Concept Space (conceptual)
% -----------------------------------------------------------------------------

\begin{figure}[t]
    \centering
    \IfFileExists{figures/language_concept_space.pdf}{
        \includegraphics[width=\textwidth]{figures/language_concept_space}
    }{
        \fbox{\parbox[c][0.22\textheight][c]{0.9\textwidth}{\centering Placeholder for conceptual concept-space diagram\\(save as \texttt{figures/language\_concept\_space.pdf})}}
    }
    \caption{\textbf{Language representations across depth (conceptual).} \textit{Left:} Early layers encode orthography and tokenization; languages cluster by script. \textit{Middle:} Mid layers form a shared semantic concept space where Indic and non-Indic languages overlap; Hindi--Urdu form a tight cluster. \textit{Right:} Late layers specialize for language-specific decoding while preserving HI--UR similarity.}
    \label{fig:concept-space}
\end{figure}

% -----------------------------------------------------------------------------
% 6. DISCUSSION
% -----------------------------------------------------------------------------

\section{Discussion}
\label{sec:discussion}

\paragraph{Detectors vs.\ generators across depth.}
Our SAE-based analysis reveals a distinction between \emph{detector} and \emph{generator} directions. Detectors---features with high monolinguality that fire selectively for specific languages---often become more numerous in later layers, while effective \emph{generator} directions for steering tend to appear in a mid-to-late window. This helps explain why monolinguality-based selectors can yield features that are predictive yet weak for generation: detectors tell the model \emph{what language is present}, not \emph{how to produce it}. We emphasize that any ``late detector peak'' is observed in the SAE decomposition; reconciling SAE-based and neuron-level language selectivity findings is an open question.

\paragraph{Language-specific patterns.}
The feature discovery data reveals distinct patterns across language families:
\begin{itemize}[leftmargin=2em]
    \item \textbf{Indo-Aryan (HI, UR, BN):} These languages show strong shared structure, with evidence for script-level effects in late layers (especially for Hindi--Urdu).
    \item \textbf{Dravidian (TA, TE):} These languages often show different selectivity profiles from Indo-Aryan, consistent with distinct scripts and families.
    \item \textbf{English:} As an English-centric pretrained model, Gemma can exhibit strong English selectivity signals in later layers.
\end{itemize}

\paragraph{Hindi--Urdu as one language.}
The high Jaccard overlap between Hindi and Urdu, combined with a comparatively small script-specific shell, is consistent with Gemma encoding a largely shared semantic subspace with orthographic specialization. The late-layer Urdu detector spike observed in our 2B runs is consistent with output-stage script disambiguation rather than semantic divergence; we treat this as suggestive rather than definitive linguistic evidence.

\paragraph{Hierarchical structure.}
Aggregating across experiments, we find a three-stage hierarchy: early layers dominated by orthography and tokenization, mid layers by shared cross-lingual semantics, and late layers by language-specific output decoding. The distinction between late \emph{detection} signals and earlier \emph{generation} directions is crucial for steering: intervening on late-layer detectors is often less effective because the generative pathway may already be committed.

% -----------------------------------------------------------------------------
% 7. LIMITATIONS
% -----------------------------------------------------------------------------

\section{Limitations and Future Work}
\label{sec:limitations}

\begin{itemize}
    \item \textbf{No human evaluation:} We rely on structural metrics and calibrated LLM judges.
    \item \textbf{Single architecture:} Gemma only; cross-architecture comparison needed.
    \item \textbf{Constant steering:} Token-aware or decaying schedules may reduce degradation.
    \item \textbf{Sample sizes:} 5000 samples for discovery, 200 prompts for evaluation---adequate for medium effect sizes but may miss rare features.
    \item \textbf{\flores{} distribution reuse:} Some experiments use FLORES for both estimation and evaluation; we enforce split separation (\texttt{dev} vs \texttt{devtest}) and remove exact overlaps, but both splits come from the same benchmark distribution.
    \item \textbf{Semantic proxy limits:} LaBSE is a proxy for semantic faithfulness. For steering prompts, our default metric is prompt-faithfulness (not full ``semantic preservation'' vs.\ baseline); we additionally report truncation rates and provide an optional baseline-preservation mode.
    \item \textbf{Language ID within scripts:} Script dominance cannot distinguish languages within Latin/Arabic scripts. We optionally use fastText-based LID for controls, but short generations remain challenging.
    \item \textbf{Causal evidence is local:} Occlusion/ablation validates a subset of features and layers; it does not establish a complete causal map.
    \item \textbf{9B scope:} 9B experiments are a sanity check with relative-depth matched probes, not an exhaustive scaling study.
    \item \textbf{Judge ground truth:} Calibration depends on heuristic ``ground truth'' labels; judge scores are supplementary evidence.
    \item \textbf{SAE vs.\ neuron-level selectivity:} Our ``late-layer detector peak'' is observed in the SAE decomposition; reconciling SAE-based and neuron-level language-selectivity findings is an open question.
\end{itemize}

% -----------------------------------------------------------------------------
% 8. CONCLUSION
% -----------------------------------------------------------------------------

\section{Conclusion}
\label{sec:conclusion}

We have presented a framework for studying multilingual representations and steering in Gemma models using Sparse Autoencoders, with a focus on Indic languages. Our analysis supports a separation between \emph{detector} features (often late and correlational) and \emph{generator} directions (mid-to-late and causally validated on a subset). Hindi--Urdu show high feature overlap together with a small script-specific shell, consistent with shared semantics and script-level specialization.

Our experimental suite, with rigorous statistical methodology, provides a reproducible foundation for future work on multilingual interpretability. Key open questions include reconciling SAE-based and neuron-level findings, developing steering schedules that minimize degradation, and extending to full multi-layer circuit reconstruction.

% -----------------------------------------------------------------------------
% ACKNOWLEDGMENTS
% -----------------------------------------------------------------------------

\section*{Acknowledgments}
We thank the Gemma Scope team for releasing open SAEs, and the creators of Samanantar, FLORES-200, MLQA, and IndicQA for their datasets.

% -----------------------------------------------------------------------------
% BIBLIOGRAPHY
% -----------------------------------------------------------------------------

\bibliographystyle{plainnat}
\bibliography{references}

% -----------------------------------------------------------------------------
% APPENDIX
% -----------------------------------------------------------------------------

\appendix

\section{Algorithm Details}
\label{app:algorithms}

\begin{algorithm}[h]
\caption{SAE-based Steering Vector Construction}
\label{alg:steering}
\begin{algorithmic}[1]
\REQUIRE Texts $\data_S$, $\data_T$; SAE at layer $\ell$; method; $k$ features
\IF{method = dense}
    \STATE $v_\ell \leftarrow \bar{h}_\ell(\data_T) - \bar{h}_\ell(\data_S)$
\ELSE
    \STATE Compute activation rates $\hat{P}_\ell(j \mid S)$, $\hat{P}_\ell(j \mid T)$
    \STATE Select top-$k$ features $S_\ell$ by chosen criterion
    \STATE $v_\ell \leftarrow \frac{1}{k}\sum_{j \in S_\ell} W_{\text{dec}}[j,:]$
\ENDIF
\STATE Normalize $v_\ell$
\RETURN $v_\ell$
\end{algorithmic}
\end{algorithm}

\section{Additional Implementation Details}
\label{app:impl}

Experiments run on NVIDIA A100 40GB GPUs with Gemma in bfloat16. SAE steering vectors are cast to model dtype to avoid precision errors. Train/test splits verified for no leakage via automated checks with data fingerprinting.

\end{document}
