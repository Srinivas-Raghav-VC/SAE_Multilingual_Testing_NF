\documentclass[11pt]{article}
\usepackage{times}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage[margin=1in]{geometry}

\title{Indic Language Representations and Steering in Gemma Models with Sparse Autoencoders}

\author{%
  Srinivas Raghav V C \\
  Indian Institute of Information Technology Kottayam \\
  \texttt{srinivas22bcs16@iiitkottayam.ac.in}
}

\date{}

\begin{document}
\maketitle

\begin{abstract}
Multilingual large language models (LLMs) are widely deployed in settings where language and script control matter, yet their internal multilingual representations and the effects of steering remain poorly understood, particularly for Indic languages. We study the Indic cluster (Hindi, Urdu, Bengali, Tamil, Telugu) in Gemma~2 models (2B and 9B)~\cite{gemma22024} using Sparse Autoencoders (SAEs) trained on Gemma residual streams, following recent work on SAE-based circuit discovery in LLMs~\cite{oneill2024sparse} and the Gemma Scope suite of SAEs~\cite{gemmascope2024}. Our contributions are fourfold. First, we characterize the hierarchical structure of Indic representations across layers, separating script-specific and semantic/generative features and analyzing cross-lingual overlaps (Hindi--Urdu vs.\ Hindi--English) using standard multilingual evaluation corpora such as FLORES~\cite{goyal2021flores}. Second, we systematically compare steering methods (dense, activation-difference, monolinguality, attribution-based) and layers across multiple Indic and non-Indic languages. Third, we quantify spillover and leakage: how steering one Indic language affects other Indic languages and unrelated controls (German, Arabic). Fourth, we evaluate robustness: how steering impacts degradation (repetition, coherence) and downstream QA performance (MLQA, IndicQA), using a calibrated Gemini-based LLM-as-judge alongside script and semantic metrics based on LaBSE~\cite{feng2020labse}. All experiments are implemented in a unified, open-source pipeline built around Gemma Scope SAEs and standard Indic datasets (Samanantar~\cite{ramesh2021samanantar}, FLORES-200, MLQA, IndicQA). This paper reports the methodology and experimental design in full; all numerical values are placeholders (N/A) pending the final execution of the pipeline.
\end{abstract}

\section{Introduction}

Multilingual LLMs are increasingly used in multilingual societies where control over language and script is critical. In the Indic context, models must handle closely related languages (e.g., Hindi and Urdu) that share semantics but differ in script, as well as diverse scripts (Devanagari, Arabic, Bengali, Tamil, Telugu). Understanding how such models internally represent these languages, and how to steer them without catastrophic degradation, is essential both for interpretability and safety.

Sparse Autoencoders (SAEs) have recently emerged as a powerful tool for interpreting LLM activations by decomposing residual stream activations into sparse, interpretable features~\cite{oneill2024sparse}. Prior work has demonstrated that some features behave as ``detectors'' (correlated with certain inputs) while others can act as ``generators'' (causally influencing outputs). However, multilingual and Indic-specific SAE analysis, especially in modern LLMs like Gemma 2, remains under-explored.

We present a comprehensive study of Indic language representations and steering in Gemma 2 (2B and 9B) using SAEs from Gemma Scope~\cite{gemmascope2024}. Our study focuses on four overarching themes:

\begin{itemize}
  \item \textbf{T1: Hierarchical geometry of Indic representations.} How are Indic languages represented across depth? Where do script-specific vs.\ semantic features live? How similar are Hindi--Urdu vs.\ Hindi--English?
  \item \textbf{T2: Steering methods and layers.} Which SAE-based steering constructions (dense, activation-difference, monolinguality, attribution-based) work best, and at what layers, for Indic language control?
  \item \textbf{T3: Spillover and leakage.} How does steering one Indic language affect other Indic languages and unrelated controls?
  \item \textbf{T4: Robustness and task transfer.} How does steering impact degradation and downstream QA performance? Can LLM-as-judge be used reliably when calibrated~\cite{krumdick2025nofreelabels}?
\end{itemize}

We implement a unified experimental framework that operationalizes these questions as a set of experiments (Exp1--Exp16) and generates both plots and a machine-readable summary for analysis. In this draft, all numerical results are left as \textbf{N/A} until the pipeline is fully executed.

\section{Related Work}

\paragraph{Multilingual representations and Indic languages.} 
Prior work has examined how multilingual transformers develop cross-lingual concept spaces, often finding shared mid-layer representations with language-specific decoding in later layers. FLORES benchmarks have become standard multilingual testbeds for evaluating translation and representation quality across many languages, including low-resource ones~\cite{goyal2021flores}. In this work we specifically use FLORES-200. Studies on Indic languages and Hindi--Urdu suggest script-specific clustering atop shared semantics. Our work builds on this by using SAEs to decompose Gemma residual streams into sparse features and explicitly separating script vs.\ semantic components.

\paragraph{Sparse Autoencoders, superposition, and steering.}
Recent SAE work has shown that language models' residual streams can be decomposed into overcomplete feature dictionaries that support circuit-level analysis and causal interventions~\cite{oneill2024sparse}. This line of work is closely tied to the superposition hypothesis~\cite{elhage2022toymodels}, which proposes that networks represent more features than they have dimensions by storing features in overlapping directions, making individual neurons polysemantic. SAEs help disentangle such superposition and expose more interpretable, sparse feature bases, and have been used to improve representational alignment measurements between models and brains~\cite{longon2025superpositionalign}. Representation engineering and steering methods (e.g., activation addition, ReFT, and related activation editing techniques) show that editing activations along carefully chosen directions can systematically change model behavior but may introduce degradation (repetition, incoherence). Our study extends these ideas to a structured, multi-language Indic setting in Gemma models, using Gemma Scope SAEs to study both language representations and steering-induced trade-offs.

\paragraph{LLM-as-judge and calibration.}
LLM-as-judge methods use large models to evaluate generated text. Recent papers show strong correlation with human judgements but also highlight significant biases and the need for calibration or human grounding~\cite{krumdick2025nofreelabels}. We adopt a calibration approach inspired by this line of work, estimating bias-corrected accuracy and confidence intervals for a Gemini-based judge.

\paragraph{Indic corpora.}
Samanantar~\cite{ramesh2021samanantar} is, to our knowledge, the largest publicly available EN--Indic parallel corpus, spanning 11 Indic languages and widely used for multilingual translation and representation learning. FLORES-200~\cite{goyal2021flores} provides high-quality parallel evaluation data for a large number of languages, including many Indic languages. We use Samanantar primarily for feature discovery and steering vector estimation, and FLORES-200 for parallel evaluation and cross-lingual analysis.

\section{Background and Hypotheses}

We consider a transformer LLM with $L$ layers and hidden dimension $d$, augmented with SAEs trained on residual stream activations for each layer. Each SAE encodes a hidden state $h \in \mathbb{R}^d$ into sparse features $z \in \mathbb{R}^m$ and reconstructs back to $\hat{h} \in \mathbb{R}^d$.

\subsection{Formalization}

Let $\mathcal{L}$ denote the set of languages we study (EN, HI, UR, BN, TA, TE, DE, AR). For an input token sequence $x_{1:T}$ and a transformer layer index $\ell \in \{0,\dots,L\}$, we write
\begin{equation}
  h_{\ell,t}(x_{1:T}) \in \mathbb{R}^d
\end{equation}
for the residual stream activation at position $t$ and layer $\ell$, and collect all positions into $H_\ell(x) \in \mathbb{R}^{T \times d}$.

For each layer $\ell$ we attach a sparse autoencoder $(f_\ell, g_\ell)$ with feature dimension $m \gg d$:
\begin{align}
  z_{\ell,t} &= f_\ell\!\bigl(h_{\ell,t}\bigr) \in \mathbb{R}^m, \\
  \hat{h}_{\ell,t} &= g_\ell(z_{\ell,t}) \in \mathbb{R}^d,
\end{align}
where $z_{\ell,t}$ is sparse (most coordinates zero) and $\hat{h}_{\ell,t}$ aims to reconstruct $h_{\ell,t}$. We denote the decoder matrix by $W^{(\ell)}_{\mathrm{dec}} \in \mathbb{R}^{m \times d}$, whose $j$-th row gives a direction in hidden space associated with feature $j$ at layer $\ell$.

For a fixed layer $\ell$ and feature index $j$, we define a binary activation indicator at token position $(x,t)$
\begin{equation}
  a_{\ell,j}(x,t) \;=\; \mathbb{I}\bigl[z_{\ell,t,j} > 0\bigr],
\end{equation}
where $\mathbb{I}[\cdot]$ is the indicator function. For a language $L \in \mathcal{L}$ with a corpus $\mathcal{D}_L$ of sentences, we estimate the \emph{activation rate} of feature $(\ell,j)$ given $L$ as
\begin{equation}
  \hat{P}_\ell(j \mid L) \;=\;
  \frac{\displaystyle \sum_{x \in \mathcal{D}_L} \sum_{t=1}^{|x|} a_{\ell,j}(x,t)}
       {\displaystyle \sum_{x \in \mathcal{D}_L} |x|},
\end{equation}
which is simply the fraction of tokens for which feature $j$ is active on layer $\ell$ for language $L$. In practice, $\mathcal{D}_L$ is drawn from Samanantar (for Indic languages with EN parallels) or FLORES-200 (for languages not covered by Samanantar).

Given these activation rates, we define a \emph{language-selectivity score} (``monolinguality'') for feature $(\ell,j)$ and target language $T \in \mathcal{L}$ as
\begin{equation}
  M_{\ell,j}(T) \;=\;
  \begin{cases}
    0, & \text{if } \hat{P}_\ell(j \mid T) < \epsilon, \\[4pt]
    \dfrac{\hat{P}_\ell(j \mid T)}{\max_{L \in \mathcal{L},\, L \neq T} \hat{P}_\ell(j \mid L)}, & \text{otherwise,}
  \end{cases}
  \label{eq:monolinguality}
\end{equation}
with a small floor $\epsilon$ (e.g., $10^{-4}$) to avoid assigning large scores to features that almost never fire on $T$. We call a feature \emph{strongly $T$-selective} if $M_{\ell,j}(T) > 3$, i.e., it is at least three times more active on $T$ than on any other language in $\mathcal{L}$. Exp1 counts such features per layer and language.

For cross-language feature overlap (Exp3, Exp5, Exp6), we work with sets of active features. For a given layer $\ell$ and language $L$, we define the set
\begin{equation}
  \mathcal{F}_\ell(L) \;=\; \bigl\{ j \in \{1,\dots,m\} : \hat{P}_\ell(j \mid L) > \tau \bigr\},
\end{equation}
where $\tau$ is an activation-rate threshold (e.g., $0.01$). For two languages $L_1, L_2$ we then compute the \emph{Jaccard overlap}
\begin{equation}
  J_\ell(L_1, L_2) \;=\;
  \frac{\bigl|\mathcal{F}_\ell(L_1) \cap \mathcal{F}_\ell(L_2)\bigr|}
       {\bigl|\mathcal{F}_\ell(L_1) \cup \mathcal{F}_\ell(L_2)\bigr|},
  \label{eq:jaccard}
\end{equation}
which is guaranteed to lie in $[0,1]$ by construction and captures how similar the active feature sets are for $L_1$ and $L_2$ at layer $\ell$.

Steering experiments (Exp2, Exp4, Exp7, Exp8, Exp9, Exp10, Exp15, Exp16) intervene on hidden states at layer $\ell$ by adding a \emph{steering vector} $v_\ell \in \mathbb{R}^d$. For a given residual stream state $h_{\ell,t}$ and scalar strength $\alpha \in \mathbb{R}$, the steered hidden state is
\begin{equation}
  \tilde{h}_{\ell,t} \;=\; h_{\ell,t} + \alpha v_\ell.
  \label{eq:steering}
\end{equation}
In the \emph{dense} case, $v_\ell$ is constructed as a difference between mean hidden states for a target language $T$ and a source language $S$:
\begin{equation}
  v_\ell^{\mathrm{dense}}(T \leftarrow S) \;=\;
  \mathbb{E}_{x \in \mathcal{D}_T}\bigl[\bar{h}_\ell(x)\bigr]
  \;-\;
  \mathbb{E}_{x \in \mathcal{D}_S}\bigl[\bar{h}_\ell(x)\bigr],
\end{equation}
where $\bar{h}_\ell(x)$ denotes the mean of $H_\ell(x)$ over tokens. In the SAE-based case, we instead select a set of features $S_\ell \subseteq \{1,\dots,m\}$ (e.g., by activation difference or monolinguality) and form
\begin{equation}
  v_\ell^{\mathrm{SAE}} \;=\;
  \frac{1}{|S_\ell|}
  \sum_{j \in S_\ell} W^{(\ell)}_{\mathrm{dec}}[j,:],
  \label{eq:sae-steering}
\end{equation}
which averages the decoder directions of selected features. Occlusion-based attribution (Exp10, Exp13) instead uses the SAE to \emph{ablate} individual or grouped features by encoding $h_{\ell,t}$, zeroing selected coordinates in $z_{\ell,t}$, and decoding back to a modified $\hat{h}_{\ell,t}$.

We will use the terms \emph{detector} and \emph{generator} feature in a precise, operational sense.

\paragraph{Detector features.}
Intuitively, a feature behaves as a detector for language $T$ if it reliably \emph{fires when text is in $T$ and remains mostly silent otherwise}. Formally, we treat the monolinguality score $M_{\ell,j}(T)$ from \eqref{eq:monolinguality} as a \emph{detection score}: features with high $M_{\ell,j}(T)$ and non-trivial activation rate on $T$ are \emph{language-selective detectors}. Exp1 and Exp3 analyse detector structure by:
\begin{itemize}
  \item counting strongly $T$-selective features ($M_{\ell,j}(T) > 3$) per layer, and
  \item comparing overlaps of active detector sets across languages via $J_\ell(L_1,L_2)$ in \eqref{eq:jaccard}.
\end{itemize}
These quantities are purely \emph{correlational}: they say when a feature is \emph{present}, not what happens if we change it.

\paragraph{Generator features.}
By contrast, a feature behaves as a generator for language $T$ if \emph{increasing its activation causally pushes the model toward generating $T$}. We formalize this via single-feature steering and occlusion. For a feature $(\ell,j)$ with decoder direction $w_{\ell,j}$, consider the intervention
\begin{equation}
  h_{\ell,t}^{(+)} \;=\; h_{\ell,t} + \alpha w_{\ell,j}, \qquad
  h_{\ell,t}^{(-)} \;=\; h_{\ell,t} - \alpha w_{\ell,j},
  \label{eq:single-feature-steer}
\end{equation}
for a small $\alpha > 0$, applied during generation. Let $y^{(+)}$ and $y^{(-)}$ be the corresponding completions for a prompt $x$, and let $S_T(y)$ denote a scalar \emph{language score} for $T$ (e.g., the fraction of alphabetic characters in the target script, or an indicator that the output passes our script-dominance test). We define a \emph{generator score} for $(\ell,j)$ with respect to $T$ as
\begin{equation}
  G_{\ell,j}(T) \;=\;
  \mathbb{E}_{x \sim \mathcal{P}}\bigl[ S_T\bigl(y^{(+)}(x)\bigr) - S_T\bigl(y^{(-)}(x)\bigr) \bigr],
  \label{eq:generator-score}
\end{equation}
where the expectation is over a prompt distribution $\mathcal{P}$ (e.g., FLORES prompts). In words, $G_{\ell,j}(T)$ measures how much nudging feature $j$ up versus down changes the tendency of the model to produce language $T$. In practice (Exp7), we approximate \eqref{eq:generator-score} using a finite set of prompts and script-based success indicators.

Occlusion-based attribution in Exp10 provides a complementary, ablation-style generator score. For feature $(\ell,j)$ we define
\begin{equation}
  A_{\ell,j}(T) \;=\;
  \mathbb{E}_{x \sim \mathcal{P}}
  \bigl[ S_T(y(x)) - S_T(\tilde{y}_{-j}(x)) \bigr],
  \label{eq:attribution-score}
\end{equation}
where $y(x)$ is the baseline completion and $\tilde{y}_{-j}(x)$ is the completion when we encode $h_{\ell,t}$ through the SAE, set $z_{\ell,t,j}\!=\!0$ for all $t$, and decode back before continuing the forward pass. A large positive $A_{\ell,j}(T)$ means that \emph{removing} feature $j$ reduces the model's ability to generate $T$; such features are strong candidates for inclusion in attribution-based steering sets $S_\ell^{\mathrm{attr}}(T,k)$.

In this paper we use:
\begin{itemize}
  \item high-$M_{\ell,j}(T)$ features as \emph{detectors} for language $T$ (Exp1, Exp3, Exp5, Exp6), and
  \item high-$G_{\ell,j}(T)$ or high-$A_{\ell,j}(T)$ features as \emph{generators} for $T$ (Exp2, Exp4, Exp7, Exp8, Exp9, Exp10, Exp15, Exp16),
\end{itemize}
with thresholds chosen empirically. This detector/generator distinction clarifies why monolinguality-based feature sets can be excellent for analysis yet poor for steering: detectors tell us \emph{when} a language is present, whereas generators tell us \emph{how to make it appear}.  

For evaluation, we combine several metrics. Script dominance is computed via Unicode-based script ratios over generated text. Semantic preservation is measured using LaBSE cosine similarity between prompt or gold-answer embeddings and generated outputs. Degradation is quantified via $n$-gram repetition rates (e.g., 3-gram and 5-gram). Finally, a calibrated Gemini-based LLM-as-judge provides language, faithfulness, and coherence scores with bias-corrected accuracy estimates~\cite{krumdick2025nofreelabels}. Subsequent sections instantiate these formal ingredients for each experiment.

We study the following grouped hypotheses:

\subsection{T1: Hierarchical Geometry of Indic Representations}

\begin{itemize}
  \item \textbf{H1: Hindi-specific detector features exist at multiple layers.} There are at least a threshold number of Hindi-specific features (monolinguality $M > 3$) per layer.
  \item \textbf{H2: Mid-layers form a shared cross-lingual space.} Shared features across languages peak in mid-layers, consistent with a ``messy middle'' concept space.
  \item \textbf{H3: Hindi--Urdu share more semantic features than Hindi--English.} Jaccard overlap of active features is higher for Hindi--Urdu than for Hindi--English at most layers.
  \item \textbf{H4: Script-specific features form a minority of Hindi--Urdu union.} Script-only features make up less than a given fraction (e.g., $< 20\%$) of the joint feature set.
\end{itemize}

\subsection{T2: Steering Methods and Layers}

\begin{itemize}
  \item \textbf{H5: Activation-difference SAE steering outperforms monolinguality.} For EN$\to$Indic steering, activation-difference SAE features yield higher target-language success than monolinguality-selected features.
  \item \textbf{H6: There exist optimal steering layers for each Indic language.} A subset of layers (often late) yield significantly higher steering success with lower degradation.
  \item \textbf{H7: Attribution-based steering improves over naive activation-diff.} Feature sets selected via occlusion-based attribution yield better steering than purely activation-difference sets.
\end{itemize}

\subsection{T3: Spillover and Leakage}

\begin{itemize}
  \item \textbf{H8: Steering a target Indic language primarily affects the Indic cluster.} EN$\to$Hindi steering increases Hindi usage and also affects other Indic languages more than non-Indic controls.
  \item \textbf{H9: Steering profile similarity is higher within the Indic cluster.} Layer-wise steering success curves are more correlated among Indic languages than between Indic and non-Indic languages.
\end{itemize}

\subsection{T4: Robustness and Task Transfer}

\begin{itemize}
  \item \textbf{H10: Steering induces measurable degradation.} Strong steering increases repetition and coherence degradation.
  \item \textbf{H11: Steering harms downstream QA performance.} Applying EN$\to$Indic steering during QA reduces QA quality (MLQA, IndicQA) relative to baseline.
  \item \textbf{H12: Calibrated LLM-as-judge yields more reliable estimates than raw scores.} Bias-corrected estimates differ from raw Gemini judge accuracy and provide tighter error bounds~\cite{krumdick2025nofreelabels}.
\end{itemize}

\section{Experimental Setup}

\subsection{Models and SAEs}

We primarily use Gemma 2B and optionally Gemma 2 9B~\cite{gemma22024}:
\begin{itemize}
  \item \textbf{Gemma 2 2B}: $L = 26$ layers, $d = 2304$.
  \item \textbf{Gemma 2 9B}: larger model with corresponding Gemma Scope SAEs.
\end{itemize}

SAEs are loaded from Gemma Scope releases and attached to residual streams at selected layers (e.g., 5, 8, 10, 13, 16, 20, 24). For each layer, the SAE has width $m = 16384$ features (width 16k), similar in spirit to SAE configurations used in prior circuit-identification work~\cite{oneill2024sparse}.

\subsection{Datasets}

\paragraph{Samanantar.}
We use the Samanantar EN--Indic parallel corpus~\cite{ramesh2021samanantar} for feature discovery and steering vector estimation, sampling up to $N_\text{train}$ sentence pairs per language. Indic languages include Hindi, Bengali, Tamil, Telugu; English serves as a pivot.

\paragraph{FLORES-200.}
FLORES-200 is a multilingual evaluation benchmark~\cite{goyal2021flores} that provides high-quality parallel data for 100+ languages. In all experiments we use FLORES-200 for:
\begin{itemize}
  \item Cross-language feature overlap and hierarchy (Exp3, Exp5, Exp6).
  \item Additional training data for languages not present in Samanantar (e.g., Urdu, German, Arabic).
\end{itemize}

\paragraph{MLQA and IndicQA.}
For downstream QA evaluation:
\begin{itemize}
  \item \textbf{MLQA}: EN, HI, DE, AR.
  \item \textbf{IndicQA}: HI, BN, TA, TE.
\end{itemize}
We use these for baseline vs.\ steered QA comparison (Exp12).

\subsection{Evaluation Metrics}

\paragraph{Script detection and code-mixing.}
We compute per-script ratios over alphabetic characters using Unicode ranges (Latin, Devanagari, Arabic, Bengali, Tamil, Telugu, etc.). A generation is classified as target script if:
\begin{enumerate}
  \item Target script ratio $\geq \tau_\text{script}$, and
  \item Target script ratio exceeds the next-largest script ratio by a dominance margin.
\end{enumerate}
We also detect code-mixing by checking if multiple scripts exceed a small threshold.

\paragraph{Semantic similarity (LaBSE).}
We use the LaBSE model~\cite{feng2020labse} to embed prompts or gold answers and generated outputs, computing cosine similarity. This is used both for steering prompts and QA answers.

\paragraph{Degradation (repetition).}
We compute 3-gram and 5-gram repetition ratios and classify outputs as degraded if either exceeds predefined thresholds.

\paragraph{LLM-as-judge (Gemini).}
We use a Gemini model as a judge with a structured rubric (language, faithfulness, coherence). Raw scores are then calibrated using a bias-correction framework inspired by recent work on limitations of LLM-as-a-judge~\cite{krumdick2025nofreelabels}, estimating sensitivity, specificity, and bias-adjusted accuracy with confidence intervals.

\section{Experiments}

In this draft, all numerical results are placeholders (N/A). The codebase provides a unified runner (\texttt{run.py}) and a summarizer (\texttt{summarize\_results.py}). Executing:
\begin{verbatim}
python run.py --all
python summarize_results.py
\end{verbatim}
will generate JSON outputs, plots, and a text summary to populate the results below.

\begin{figure}[t]
  \centering
  % Save the infographic image as, e.g., figures/indic_sae_infographic.(pdf|png|jpg)
  \includegraphics[width=\textwidth]{figures/indic_sae_infographic}
  \caption{High-level overview of our experimental setup. Left: Gemma 2B/9B transformer with Gemma Scope SAEs attached to residual streams at selected layers, processing inputs in multiple scripts (Latin, Devanagari, Arabic, Bengali, Tamil, Telugu) using Samanantar, FLORES-200, MLQA, and IndicQA. Middle: feature analysis via monolinguality-based detectors (Exp1), Hindi--Urdu overlap and script vs semantic splits (Exp3, Exp6), and cross-layer EN--HI alignment (Exp14). Right: steering and evaluation pipeline, including EN$\to$Indic steering with dense and SAE-based directions (Exp2, Exp7, Exp9, Exp10), spillover analysis (Exp4), scaling to 9B (Exp8), QA degradation (Exp12), and calibrated LLM-as-judge evaluation (Exp11), together with robustness metrics (script dominance, LaBSE semantics, repetition).}
  \label{fig:overview}
\end{figure}

\subsection{Exp1: Feature Discovery (Monolinguality Detectors)}

\textbf{Goal.} Identify language-selective detector features by computing monolinguality scores per SAE feature and language, and test whether such features peak in mid layers or late layers.

\textbf{Method.} For each feature $j$ and language $L$, we estimate an activation rate $P(j \mid L)$ as the fraction of tokens on which $j$ is active in Samanantar/FLORES samples. For a target language $T$, we define a monolinguality score
\begin{equation}
  M_j(T) \;=\; \frac{P(j \mid T)}{\max_{L \neq T} P(j \mid L)},
\end{equation}
zeroing out features that almost never fire on $T$ (very small $P(j \mid T)$). A feature is counted as \emph{strongly $T$-selective} if $M_j(T) > 3$, i.e., it is at least three times more active on $T$ than on any other language in our set (EN, HI, UR, BN, TA, TE, DE, AR). We then count, for each layer, how many features are strongly selective for \emph{any} language under this definition.

Our original hypothesis (H3) was that language-specific detector features would peak in a ``mid'' band (layers 10--16), reflecting a mid-layer ``messy middle'' where language identity is maximally separable before decoding. In contrast, preliminary results on Gemma~2B show that the total number of strongly language-selective features \emph{increases toward the top of the network}, with a peak at the final SAE layer (e.g., one run yields 797, 764, 884, 777, 1084, 1506, 1918 strongly selective features at layers 5, 8, 10, 13, 16, 20, 24 respectively). Under this detector metric, H3 is \emph{falsified}: language selectivity is strongest in late layers rather than in the 10--16 band.

\textbf{Interpretation.} This pattern suggests that, in Gemma~2B, SAE features that behave as \emph{language or script detectors} are concentrated in late residual layers closer to the logits. Mid layers may still host a more language-agnostic semantic space (as probed in Exp14), but the features that are most sharply tuned to specific languages emerge in the later ``decoding'' region. This is consistent with our steering experiments (Exp2/Exp9), where late-layer interventions tend to produce the strongest monolingual steering effects.

\textbf{Results (placeholder).}
\begin{itemize}
  \item Strongly language-selective features (M $>3$) per layer: \textbf{N/A} (example: 797, 764, 884, 777, 1084, 1506, 1918 for layers 5, 8, 10, 13, 16, 20, 24).
  \item Dead features per language per layer: \textbf{N/A}.
\end{itemize}

\subsection{Exp3: Hindi--Urdu Overlap (Correct Jaccard)}

\textbf{Goal.} Quantify overlap between Hindi and Urdu features and separate script vs.\ semantic features.

\textbf{Method.} For each layer, we collect active features for HI, UR, EN on FLORES and compute Jaccard overlaps and script/semantic ratios using consistent thresholds.

\textbf{Results (placeholder).}
\begin{itemize}
  \item J(HI,UR) per layer: \textbf{N/A}.
  \item J(HI,EN) per layer: \textbf{N/A}.
  \item Semantic fraction of HI--UR union: \textbf{N/A}.
  \item Script-specific fraction of HI--UR union: \textbf{N/A}.
\end{itemize}

\subsection{Exp5 \& Exp6: Hierarchical and Script vs Semantic Structure}

\textbf{Goal.} Analyze shared vs Indic-only vs script/semantic features across layers, and cluster Indic vs non-Indic languages.

\textbf{Method.} We compute per-language feature sets, overlaps, Indic-only sets, and classify features as script-only or semantic (Exp6), summarizing early, mid, late bands.

\textbf{Results (placeholder).}
\begin{itemize}
  \item Shared feature counts: \textbf{N/A}.
  \item Indic-only feature counts: \textbf{N/A}.
  \item Early/mid/late Hindi--Urdu overlap averages: \textbf{N/A}.
  \item Script-only vs semantic feature counts per layer: \textbf{N/A}.
\end{itemize}

\subsection{Exp2, Exp7, Exp9, Exp10: Steering Methods and Layers}

\textbf{Goal.} Compare steering methods (dense, activation-diff SAE, monolinguality SAE, random, attribution-based) across layers and languages.

\textbf{Method.}
\begin{itemize}
  \item \textbf{Exp2:} Compare methods at selected layers (e.g., 5, 13, 20, 24) for EN$\to$HI steering.
  \item \textbf{Exp7:} Probe individual SAE features by steering them up/down and measuring changes in script ratio, semantic similarity, and degradation.
  \item \textbf{Exp9:} Full layer $\times$ method $\times$ language sweep (HI, BN, TA, TE, UR, DE, AR), evaluating multiple strengths and aggregating script, semantics, degradation, and LLM-judge scores.
  \item \textbf{Exp10:} Use occlusion attribution to identify causally important features and build an attribution-based steering vector, comparing it to dense and activation-diff vectors.
\end{itemize}

\textbf{Results (placeholder).}
\begin{itemize}
  \item Best success (script+semantic) per language and method, with layer and strength: \textbf{N/A}.
  \item Attribution vs activation-diff vs dense steering metrics (success \%, degradation \%): \textbf{N/A}.
  \item Distribution of feature-level causal effects (Exp7): \textbf{N/A}.
\end{itemize}

\subsection{Exp4: Spillover Analysis}

\textbf{Goal.} Measure spillover: how EN$\to$Hindi steering affects other Indic languages vs non-Indic controls.

\textbf{Method.} Apply a Hindi steering vector and classify outputs by script/language, measuring probabilities of outputs in HI, UR, BN, TA, TE, DE, AR.

\textbf{Results (placeholder).}
\begin{itemize}
  \item Spillover rates to each language under Hindi steering: \textbf{N/A}.
\end{itemize}

\subsection{Exp8: Scaling to 9B and Low-Resource Languages}

\textbf{Goal.} Compare 2B vs 9B representations and simple EN$\to$HI steering, including low-resource Indic and non-Indic languages.

\textbf{Method.} Compute active feature counts per language and layer, and evaluate dense vs SAE steering on a small set of prompts for EN$\to$HI.

\textbf{Results (placeholder).}
\begin{itemize}
  \item Feature coverage for 2B vs 9B per language and layer: \textbf{N/A}.
  \item EN$\to$HI success and degradation for 2B vs 9B layers: \textbf{N/A}.
\end{itemize}

\subsection{Exp11: Calibrated LLM-as-Judge}

\textbf{Goal.} Calibrate a Gemini-based judge for multilingual steering evaluation.

\textbf{Method.} Use a calibration set with ground-truth labels; estimate sensitivity, specificity, and bias-corrected accuracy with CIs as in recent LLM-as-judge analyses~\cite{krumdick2025nofreelabels}.

\textbf{Results (placeholder).}
\begin{itemize}
  \item Raw judge accuracy per language: \textbf{N/A}.
  \item Bias-corrected accuracy and 95\% CI per language: \textbf{N/A}.
\end{itemize}

\subsection{Exp12: QA Degradation Under Steering}

\textbf{Goal.} Quantify how EN$\to$target steering affects QA performance on MLQA and IndicQA.

\textbf{Method.} For each target language (HI, DE, AR; HI, BN, TA, TE), we:
\begin{itemize}
  \item Use Exp9 results to pick best steering layer, strength, and method.
  \item Generate baseline and steered answers for MLQA/IndicQA questions.
  \item Evaluate script dominance, LaBSE similarity to gold answers, degradation, and LLM-judge scores.
\end{itemize}

\textbf{Results (placeholder).}
\begin{itemize}
  \item MLQA/IndicQA baseline vs steered script success, semantic similarity, and degradation per language: \textbf{N/A}.
\end{itemize}

\section{Discussion}

In this draft, we have described a comprehensive experimental framework for analyzing Indic language representations and steering in Gemma models using SAEs. Once executed, this framework will support:

\begin{itemize}
  \item A hierarchical picture of where script vs semantic features and cross-lingual overlaps live.
  \item A detailed comparison of steering methods and layers across the Indic cluster and controls.
  \item Quantified spillover and leakage when steering a single Indic language.
  \item A systematic assessment of steering-induced degradation, both in free-form generation and QA.
\end{itemize}

All numerical results are currently \textbf{N/A}; the next step is to execute the full pipeline (e.g., \texttt{python run.py --all} followed by \texttt{python summarize\_results.py}) and populate the tables and figures.

\section{Limitations and Future Work}

\begin{itemize}
  \item \textbf{No human-annotated evaluation yet.} We rely on structural metrics, LaBSE, and a calibrated Gemini judge. Human evaluation is needed to validate conclusions.
  \item \textbf{Single architecture.} We focus on Gemma 2 (2B, 9B). Cross-architecture comparison (e.g., LLaMA, Qwen) is left for future work.
  \item \textbf{Single steering schedule.} We use constant-strength steering. More sophisticated schedules (e.g., decaying or token-aware) may reduce degradation.
  \item \textbf{Partial circuit view.} While we probe individual features and do occlusion attribution at single layers, we do not yet reconstruct full multi-layer circuits.
\end{itemize}

\section{Conclusion}

We have presented a unified, research-grade experimental framework for studying Indic language representations and steering in Gemma models using SAEs, covering hierarchy, steering methods and layers, spillover, and robustness. Once populated with data, this framework will support a detailed preprint on how Indic languages are encoded and controlled in Gemma, and how steering trades off control against degradation.

\bibliographystyle{plain}
\bibliography{references}

\end{document}
