@inproceedings{ramesh2021samanantar,
  title        = {Samanantar: The Largest Publicly Available Parallel Corpora Collection for 11 Indic Languages},
  author       = {Gowtham Ramesh and Sumanth Doddapaneni and Aravinth Bheemaraj and Mayank Jobanputra and Raghavan A K and Ajitesh Sharma and Sujit Sahoo and Harshita Diddee and Mahalakshmi J and Divyanshu Kakwani and Navneet Kumar and Aswin Pradeep and Srihari Nagaraj and Kumar Deepak and Vivek Raghavan and Anoop Kunchukuttan and Pratyush Kumar and Mitesh Shantadevi Khapra},
  booktitle    = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  year         = {2021},
  archivePrefix= {arXiv},
  eprint       = {2104.05596},
  primaryClass = {cs.CL}
}

@inproceedings{goyal2021flores,
  title        = {The {FLORES} Evaluation Benchmark for Low-Resource and Multilingual Machine Translation},
  author       = {Naman Goyal and Cynthia Gao and Vishrav Chaudhary and Peng-Jen Chen and Guillaume Wenzek and Da Ju and Sanjana Krishnan and Marc'Aurelio Ranzato and Francisco Guzm{\'a}n and Angela Fan},
  booktitle    = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics},
  year         = {2021},
  archivePrefix= {arXiv},
  eprint       = {2106.03193},
  primaryClass = {cs.CL}
}

@article{feng2020labse,
  title        = {Language-agnostic {BERT} Sentence Embedding},
  author       = {Feng, Fangxiaoyu and Yang, Yinfei and Cer, Daniel and Arivazhagan, Naveen and Wang, Wei},
  journal      = {arXiv preprint arXiv:2007.01852},
  year         = {2020}
}

@inproceedings{lewis2019mlqa,
  title        = {{MLQA}: Evaluating Cross-lingual Extractive Question Answering},
  author       = {Patrick Lewis and Barlas O{\u{g}}uz and Ruty Rinott and Sebastian Riedel and Holger Schwenk},
  booktitle    = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  year         = {2020},
  archivePrefix= {arXiv},
  eprint       = {1910.07475},
  primaryClass = {cs.CL}
}

@article{gemma2024models,
  title        = {Gemma: Open Models Based on Gemini Research and Technology},
  author       = {{Gemma Team}},
  journal      = {arXiv preprint arXiv:2403.08295},
  year         = {2024}
}

@article{gemma22024,
  title        = {Gemma 2: Improving Open Language Models at a Practical Scale},
  author       = {{Gemma 2 Team}},
  journal      = {arXiv preprint arXiv:2408.00118},
  year         = {2024}
}

@article{gemmascope2024,
  title        = {Gemma Scope: Open Sparse Autoencoders Everywhere All At Once on Gemma 2},
  author       = {Tom Lieberum and Senthooran Rajamanoharan and Arthur Conmy and Lewis Smith and Nicolas Sonnerat and Vikrant Varma and J{\'a}nos Kram{\'a}r and Anca Dragan and Rohin Shah and Neel Nanda},
  journal      = {arXiv preprint arXiv:2408.05147},
  year         = {2024}
}

@article{oneill2024sparse,
  title        = {Sparse Autoencoders Enable Scalable and Reliable Circuit Identification in Language Models},
  author       = {Charles O'Neill and Thang Bui},
  journal      = {arXiv preprint arXiv:2405.12522},
  year         = {2024}
}

@article{elhage2022toymodels,
  title        = {Toy Models of Superposition},
  author       = {Nelson Elhage and Tristan Hume and Catherine Olsson and Nicholas Schiefer and Tom Henighan and Shauna Kravec and Zac Hatfield-Dodds and Robert Lasenby and Dawn Drain and Carol Chen and Roger Grosse and Sam McCandlish and Jared Kaplan and Dario Amodei and Martin Wattenberg and Christopher Olah},
  journal      = {arXiv preprint arXiv:2209.10652},
  year         = {2022}
}

@article{longon2025superpositionalign,
  title        = {Superposition Disentanglement of Neural Representations Reveals Hidden Alignment},
  author       = {Longon, Andr{\'e} and Klindt, David and Khosla, Meenakshi},
  journal      = {arXiv preprint arXiv:2510.03186},
  year         = {2025}
}

@article{chou2025causal,
  title        = {Causal Language Control in Multilingual Transformers via Sparse Feature Steering},
  author       = {Chou, Cheng-Ting and Liu, George and Sun, Jessica and Blondin, Cole and Zhu, Kevin and Sharma, Vasu and O'Brien, Sean},
  journal      = {arXiv preprint arXiv:2507.13410},
  year         = {2025}
}

@article{cho2025corrsteer,
  title        = {{CorrSteer}: Steering Improves Task Performance and Safety in {LLM}s through Correlation-based Sparse Autoencoder Feature Selection},
  author       = {Cho, Seonglae and Wu, Zekun and Koshiyama, Adriano},
  journal      = {arXiv preprint arXiv:2508.12535},
  year         = {2025}
}

@article{krumdick2025nofreelabels,
  title        = {No Free Labels: Limitations of {LLM}-as-a-Judge Without Human Grounding},
  author       = {Michael Krumdick and Charles Lovering and Varshini Reddy and Seth Ebner and Chris Tanner},
  journal      = {arXiv preprint arXiv:2503.05061},
  year         = {2025}
}

@article{turner2023activation,
  title        = {Activation Addition: Steering Language Models Without Optimization},
  author       = {Turner, Alexander Matt and Thiergart, Lisa and Udell, David and Leech, Gavin and Mini, Ulisse and MacDiarmid, Monte},
  journal      = {arXiv preprint arXiv:2308.10248},
  year         = {2023}
}

@article{templeton2024scaling,
  title        = {Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet},
  author       = {Templeton, Adly and Conerly, Tom and Marcus, Jonathan and Lindsey, Jack and Bricken, Trenton and Chen, Brian and Pearce, Adam and Citro, Craig and Ameisen, Emmanuel and Jones, Andy and Cunningham, Hoagy and Turner, Nicholas L and McDougall, Callum and MacDiarmid, Monte and Freeman, C Daniel and Sumers, Theodore R and Rees, Edward and Batson, Joshua and Jermyn, Adam and Carter, Shan and Olah, Chris and Henighan, Tom},
  journal      = {Transformer Circuits Thread},
  year         = {2024},
  note         = {Anthropic}
}

@article{wendler2024llamas,
  title        = {Do Llamas Work in English? On the Latent Language of Multilingual Transformers},
  author       = {Wendler, Chris and Veselovsky, Veniamin and Monea, Giovanni and West, Robert},
  journal      = {arXiv preprint arXiv:2402.10588},
  year         = {2024}
}

@inproceedings{kakwani2020indicnlpsuite,
  title        = {IndicNLPSuite: Monolingual Corpora, Evaluation Benchmarks and Pre-trained Multilingual Language Models for Indian Languages},
  author       = {Kakwani, Divyanshu and Kunchukuttan, Anoop and Golla, Satish and Gokul, N C and Bhattacharyya, Avik and Khapra, Mitesh M and Kumar, Pratyush},
  booktitle    = {Findings of EMNLP},
  year         = {2020}
}

@article{khanuja2021muril,
  title        = {{MuRIL}: Multilingual Representations for Indian Languages},
  author       = {Khanuja, Simran and Bansal, Diksha and Mehtani, Sarvesh and Khosla, Savya and Dey, Atreyee and Gopalan, Balaji and Margam, Dilip Kumar and Agarwal, Partha and Shrivastava, Rajiv and Jain, Shalini and others},
  journal      = {arXiv preprint arXiv:2103.10730},
  year         = {2021}
}

@conference{dorner2024limits,
  title        = {Limits to Scalable Evaluation at the Frontier: {LLM} as Judge Won't Beat Twice the Data},
  author       = {Dorner, Florian E. and Nastl, Vivian Y. and Hardt, Moritz},
  booktitle    = {International Conference on Learning Representations (ICLR)},
  year         = {2025},
  note         = {arXiv:2410.13341}
}

% ============================================================================
% Additional citations for comprehensive context
% ============================================================================

@article{bricken2023monosemanticity,
  title        = {Towards Monosemanticity: Decomposing Language Models With Dictionary Learning},
  author       = {Bricken, Trenton and Templeton, Adly and Batson, Joshua and Chen, Brian and Jermyn, Adam and Conerly, Tom and Turner, Nick and Anil, Cem and Denison, Carson and Askell, Amanda and Lasenby, Robert and Wu, Yifan and Kravec, Shauna and Schiefer, Nicholas and Maxwell, Tim and Joseph, Nicholas and Hatfield-Dodds, Zac and Tamkin, Alex and Nguyen, Karina and McLean, Brayden and Burke, Josiah E and Hume, Tristan and Carter, Shan and Henighan, Tom and Olah, Christopher},
  journal      = {Transformer Circuits Thread},
  year         = {2023},
  note         = {Anthropic}
}

@article{cunningham2023sparse,
  title        = {Sparse Autoencoders Find Highly Interpretable Features in Language Models},
  author       = {Cunningham, Hoagy and Ewart, Aidan and Riggs, Logan and Huben, Robert and Sharkey, Lee},
  journal      = {arXiv preprint arXiv:2309.08600},
  year         = {2023}
}

@article{marks2024sparse,
  title        = {Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models},
  author       = {Marks, Samuel and Rager, Can and Michaud, Eric J and Belinkov, Yonatan and Bau, David and Mueller, Aaron},
  journal      = {arXiv preprint arXiv:2403.19647},
  year         = {2024}
}

@article{conmy2023automated,
  title        = {Towards Automated Circuit Discovery for Mechanistic Interpretability},
  author       = {Conmy, Arthur and Mavor-Parker, Augustine N and Lynch, Aengus and Heimersheim, Stefan and Garriga-Alonso, Adria},
  journal      = {Advances in Neural Information Processing Systems},
  volume       = {36},
  year         = {2023}
}

@article{pires2019multilingual,
  title        = {How Multilingual is Multilingual {BERT}?},
  author       = {Pires, Telmo and Schlinger, Eva and Garrette, Dan},
  journal      = {arXiv preprint arXiv:1906.01502},
  year         = {2019}
}

@article{wu2019beto,
  title        = {Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of {BERT}},
  author       = {Wu, Shijie and Dredze, Mark},
  journal      = {arXiv preprint arXiv:1904.09077},
  year         = {2019}
}

@article{conneau2020emerging,
  title        = {Emerging Cross-lingual Structure in Pretrained Language Models},
  author       = {Conneau, Alexis and Wu, Shijie and Li, Haoran and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal      = {arXiv preprint arXiv:1911.01464},
  year         = {2020}
}

@article{chi2020finding,
  title        = {Finding Universal Grammatical Relations in Multilingual {BERT}},
  author       = {Chi, Ethan A and Hewitt, John and Manning, Christopher D},
  journal      = {arXiv preprint arXiv:2005.04511},
  year         = {2020}
}

@article{libovicky2020language,
  title        = {Language Neutrality in Multilingual Representations},
  author       = {Libovick{\'y}, Jind{\v{r}}ich and Rosa, Rudolf and Fraser, Alexander},
  journal      = {arXiv preprint arXiv:1912.07076},
  year         = {2020}
}

@article{doddapaneni2021primer,
  title        = {A Primer on Pretrained Multilingual Language Models},
  author       = {Doddapaneni, Sumanth and Ramesh, Gowtham and Khapra, Mitesh M and Kunchukuttan, Anoop and Kumar, Pratyush},
  journal      = {arXiv preprint arXiv:2107.00676},
  year         = {2021}
}

@article{zhang2024towards,
  title        = {Towards Best Practices of Activation Patching in Language Models: Metrics and Methods},
  author       = {Zhang, Fred and Nanda, Neel},
  journal      = {arXiv preprint arXiv:2309.16042},
  year         = {2024}
}

@article{wang2022interpretability,
  title        = {Interpretability in the Wild: A Circuit for Indirect Object Identification in {GPT-2} Small},
  author       = {Wang, Kevin and Variengien, Alexandre and Conmy, Arthur and Shlegeris, Buck and Steinhardt, Jacob},
  journal      = {arXiv preprint arXiv:2211.00593},
  year         = {2022}
}

@article{geva2023dissecting,
  title        = {Dissecting Recall of Factual Associations in Auto-Regressive Language Models},
  author       = {Geva, Mor and Bastings, Jasmijn and Filippova, Katja and Globerson, Amir},
  journal      = {arXiv preprint arXiv:2304.14767},
  year         = {2023}
}

@article{zou2023representation,
  title        = {Representation Engineering: A Top-Down Approach to {AI} Transparency},
  author       = {Zou, Andy and Phan, Long and Chen, Sarah and Campbell, James and Guo, Phillip and Ren, Richard and Pan, Alexander and Yin, Xuwang and Mazeika, Mantas and Dombrowski, Ann-Kathrin and others},
  journal      = {arXiv preprint arXiv:2310.01405},
  year         = {2023}
}

@article{burns2022discovering,
  title        = {Discovering Latent Knowledge in Language Models Without Supervision},
  author       = {Burns, Collin and Ye, Haotian and Klein, Dan and Steinhardt, Jacob},
  journal      = {arXiv preprint arXiv:2212.03827},
  year         = {2022}
}

@article{li2024inference,
  title        = {Inference-Time Intervention: Eliciting Truthful Answers from a Language Model},
  author       = {Li, Kenneth and Oikarinen, Oam and Peng, Sheng and Weng, Tong and Chen, Jeffrey and Chen, Liang-Yan and others},
  journal      = {Advances in Neural Information Processing Systems},
  volume       = {36},
  year         = {2024}
}

@article{jain2023mechanistically,
  title        = {Mechanistically Analyzing the Effects of Fine-Tuning on Procedurally Defined Tasks},
  author       = {Jain, Samyak and Kirk, Robert and Lubana, Ekdeep Singh and Dick, Robert P and Tanaka, Hidenori and Grefenstette, Edward and Rockt{\"a}schel, Tim and Krueger, David Scott},
  journal      = {arXiv preprint arXiv:2311.12786},
  year         = {2023}
}

@article{rajaee2022isotropy,
  title        = {An Isotropy Analysis in the Multilingual {BERT} Embedding Space},
  author       = {Rajaee, Sara and Pilehvar, Mohammad Taher},
  journal      = {arXiv preprint arXiv:2110.08033},
  year         = {2022}
}

@inproceedings{clark2019does,
  title        = {What Does {BERT} Look At? An Analysis of {BERT}'s Attention},
  author       = {Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D},
  booktitle    = {Proceedings of the 2019 ACL Workshop BlackboxNLP},
  year         = {2019}
}

@article{stolfo2023mechanistic,
  title        = {A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis},
  author       = {Stolfo, Alessandro and Belinkov, Yonatan and Sachan, Mrinmaya},
  journal      = {arXiv preprint arXiv:2305.15054},
  year         = {2023}
}

@article{hanna2023how,
  title        = {How does {GPT-2} compute greater-than?: Interpreting mathematical abilities in a pre-trained language model},
  author       = {Hanna, Michael and Liu, Ollie and Variengien, Alexandre},
  journal      = {Advances in Neural Information Processing Systems},
  volume       = {36},
  year         = {2023}
}
